{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e46def-7e3c-44b2-9814-619bcf1683cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import pickle\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13ae91c0-c7fb-4797-9a96-dc1539ec57e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this function is copied from HMM.ipynb\n",
    "# array: numpy array\n",
    "# flank: positive integer\n",
    "def sliding_window(array, flank):\n",
    "    assert flank > 0\n",
    "    assert type(array) is np.ndarray\n",
    "    assert np.logical_not(np.isnan(np.sum(array)))\n",
    "    nrow = array.shape[0]\n",
    "    assert nrow > 0\n",
    "    ncol = array.shape[1]\n",
    "    assert ncol > 0\n",
    "    res = np.empty(shape=(nrow, (2*flank+1)*ncol))\n",
    "    res[:] = np.nan\n",
    "    for i in list(range(0,nrow)):\n",
    "        s, e = i-flank, i+flank+1\n",
    "        k = 0;\n",
    "        for j in list(range(s,e)):\n",
    "            if (j < 0 or j >= nrow):\n",
    "                res[i, k:k+ncol] = 0\n",
    "            else:\n",
    "                assert np.logical_not(np.isnan(np.sum(array[j])))\n",
    "                assert array[j].shape == (ncol,)\n",
    "                res[i, k:k+ncol] = array[j]\n",
    "            k += ncol\n",
    "    assert np.logical_not(np.isnan(np.sum(res)))\n",
    "    assert res.shape == (nrow, (2*flank+1)*ncol)\n",
    "    return res\n",
    "\n",
    "# this function rounds predictions into 1 and 0s\n",
    "def argmax(arr):\n",
    "    n, c = arr.shape\n",
    "    assert c == 3\n",
    "    assert type(arr) is np.ndarray\n",
    "    assert np.logical_not(np.isnan(np.sum(arr)))\n",
    "    res = np.empty(shape=(n,c))\n",
    "    res[:] = np.nan\n",
    "    for i in list(range(0,n)):\n",
    "        max_idx = np.argmax(arr[i])\n",
    "        if max_idx == 0:\n",
    "            res[i] = np.array([1, 0, 0])\n",
    "        elif max_idx == 1:\n",
    "            res[i] = np.array([0, 1, 0])\n",
    "        else:\n",
    "            assert max_idx == 2\n",
    "            res[i] = np.array([0, 0, 1])\n",
    "    assert np.logical_not(np.isnan(np.sum(res)))\n",
    "    return res\n",
    "\n",
    "def get_dssp(ID):\n",
    "    path = '/homes/adrozdetskiy/Projects/JnetDatasets/DSSP_out/' + ID + '.sec'\n",
    "    ls = list(pd.read_csv(path).loc[0].values)\n",
    "    ls[0] = ls[0][-1:] # remove the DSSP: part from first list item\n",
    "    res = np.empty(shape=(len(ls),3))\n",
    "    res[:] = np.nan\n",
    "    for i in range(0,len(ls)):\n",
    "        if ls[i] == 'H':\n",
    "            res[i] = np.array([1,0,0])\n",
    "        else:\n",
    "            if ls[i] == 'E' or ls[i] == 'B':\n",
    "                res[i] = np.array([0,1,0])\n",
    "            else:\n",
    "                assert ls[i] != None\n",
    "                res[i] = np.array([0,0,1])\n",
    "    assert not np.isnan(np.sum(res))\n",
    "    return res\n",
    "\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)\n",
    "    \n",
    "# given the prediction and truth for a sequence\n",
    "# write a function to calculate 0 Hcorrect, 1 Htotal, 2 Ecorrect, 3 Etotal, 4 Ccorrect, 5 Ctotal, 6 Ncorrect, 7 Ntotal\n",
    "# and return it as a np array\n",
    "# e.g. seqID 21690\n",
    "# require numpy as np\n",
    "def get_Qall(pred_ndarr, truth_ndarr):\n",
    "    assert type(pred_ndarr)==np.ndarray\n",
    "    assert type(truth_ndarr)==np.ndarray\n",
    "    assert pred_ndarr.shape==truth_ndarr.shape\n",
    "\n",
    "    # subroutine to update a np.ndarray of shape (1,8) given a single digit prediction\n",
    "    def update(pred_arr,truth_arr,res_arr):\n",
    "        pred = np.argmax(pred_arr)\n",
    "        truth = np.argmax(truth_arr)\n",
    "        if pred==truth:\n",
    "            res_arr[0,6] +=1 # increment Ncorrect\n",
    "            res_arr[0,pred+truth] +=1 # increment H/E/C-correct\n",
    "            res_arr[0,2*truth+1] +=1 # increment H/E/C-total\n",
    "        else:\n",
    "            res_arr[0,2*truth+1] += 1 # increment H/E/C-total\n",
    "        res_arr[0,7] += 1 # increment Ntotal\n",
    "        return res_arr\n",
    "    \n",
    "    results = np.zeros((1,8))\n",
    "    for (pred_arr, truth_arr) in zip(pred_ndarr, truth_ndarr):\n",
    "        results = update(pred_arr, truth_arr, results)\n",
    "    \n",
    "    assert results[0,0]+results[0,2]+results[0,4]==results[0,6]\n",
    "    assert results[0,1]+results[0,3]+results[0,5]==results[0,7]\n",
    "    \n",
    "    return results\n",
    "\n",
    "#convert (1,3) to a character. works for both dssp and prediction\n",
    "def collapse(arr):\n",
    "    assert type(arr)==np.ndarray\n",
    "    i = np.argmax(arr)\n",
    "    assert i<3;\n",
    "    assert i>=0;\n",
    "    if (i==0):\n",
    "        return 'H'\n",
    "    elif (i==1):\n",
    "        return 'E'\n",
    "    else:\n",
    "        return 'C'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7270a19d-217f-4598-9d46-ac8b23434a32",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create SOV files for HMM 2-layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1662f75-a347-43ef-a4ed-bf387cfa1c17",
   "metadata": {},
   "source": [
    "## Save predictions for all 7 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f86a35-e561-4d25-9882-8f9ebaf30abc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished saving predictions for fold 1\n",
      "\n",
      "Finished saving predictions for fold 2\n",
      "\n",
      "Finished saving predictions for fold 3\n",
      "\n",
      "Finished saving predictions for fold 4\n",
      "\n",
      "Finished saving predictions for fold 5\n",
      "\n",
      "Finished saving predictions for fold 6\n",
      "\n",
      "Finished saving predictions for fold 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first make sure that output path is valid, otherwise computation will go to waste\n",
    "out_paths = [\"/cluster/gjb_lab/2472402/outputs/hmm_cross_val/sov/cv%d/\" % num for num in range(1,8)]\n",
    "assert all([path.exists(out_path) & (out_path[-1] == '/') for out_path in out_paths])\n",
    "\n",
    "# path to training and validation examples\n",
    "paths = [\"/homes/adrozdetskiy/Projects/JnetDatasets/Jnet_training_output_v2/cross-val%d/\" % num for num in range(1,8)]\n",
    "\n",
    "# sequence dictionary or sd. need this to retrieve dssp information\n",
    "sd = pickle.load(open('/cluster/gjb_lab/2472402/data/cross-val/cross_val_dict.pkl','rb')) \n",
    "\n",
    "\n",
    "# for each fold in the 7 fold cross validation procedure, do...\n",
    "for counter, (out_path,p) in enumerate(zip(out_paths,paths)):\n",
    "\n",
    "    counter += 1 # start from 1\n",
    "    \n",
    "    valid_path = p + 'valid/'\n",
    "    \n",
    "    # get file names ending with .hmm\n",
    "    valid_files = [f for f in os.listdir(valid_path) if f[-4:]=='.hmm']\n",
    "    \n",
    "    # read profile hmms as numpy arrays\n",
    "    valid_hmm = [np.genfromtxt(fname = valid_path + fn) for fn in valid_files]\n",
    "    \n",
    "    # window profile hmms to get patterns\n",
    "    # layer 1 sliding window flank = 8\n",
    "    X_valid = [sliding_window(hmm, flank=8) for hmm in valid_hmm]\n",
    "    del valid_hmm\n",
    "    \n",
    "    # get DSSP information \n",
    "    \n",
    "    # remove .hmm extension and convert to int for lookup\n",
    "    valid_numbers = [int(fn[:-4]) for fn in valid_files] \n",
    "    \n",
    "    # lookup seqIDs given jnet number\n",
    "    valid_dssp_IDs = [sd[sd.number == num].letters.values[0] for num in valid_numbers]\n",
    "    \n",
    "    # obtain 3-column DSSP of domains given their seqID. See get_dssp() defined above\n",
    "    Y_valid = [get_dssp(ID) for ID in valid_dssp_IDs]\n",
    "\n",
    "    assert all([y.shape[1] == 3 for y in Y_valid])\n",
    "    assert all([x.shape[0] == y.shape[0] for (x, y) in zip(X_valid, Y_valid)])\n",
    "    \n",
    "    # generate layer 1 predictions\n",
    "    \n",
    "    model1 = keras.models.load_model('/cluster/gjb_lab/2472402/outputs/hmm_cross_val/fold%d_model1' % counter)\n",
    "\n",
    "    Y_pred_1 = [argmax(model1.predict(X)) for X in X_valid]\n",
    "    \n",
    "    # convert layer 1 predictions into layer 2 input\n",
    "    X_valid_2 = [sliding_window(Y, flank = 9) for Y in Y_pred_1]\n",
    "    \n",
    "    # discard model object (and its input) from RAM\n",
    "    del model1\n",
    "    del Y_pred_1\n",
    "    \n",
    "    # sanity checks before passing into model2\n",
    "    assert all([X.shape[1] == 57 for X in X_valid_2])\n",
    "    assert all([X.dtype == 'float64' for X in X_valid_2])\n",
    "    \n",
    "    # generate layer 2 predictions\n",
    "    \n",
    "    model2 = keras.models.load_model('/cluster/gjb_lab/2472402/outputs/hmm_cross_val/fold%d_model2' % counter)\n",
    "\n",
    "    Y_pred_2 = [model2.predict(X) for X in X_valid_2]\n",
    "    \n",
    "    # discard model object (and its input) from RAM\n",
    "    del model2\n",
    "    del X_valid_2\n",
    "    \n",
    "    # write SOV input into file\n",
    "    # read fasta sequence\n",
    "    \n",
    "    pickle.dump([valid_numbers, valid_dssp_IDs, Y_valid, Y_pred_2], open(out_path+\"input.pkl\", 'wb'))\n",
    "    print(\"Finished saving predictions for fold %d\\n\" % counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb687088-915f-4d52-976a-68cd1012b107",
   "metadata": {},
   "source": [
    "Replace pickle part with this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed54bf-7518-4a94-905b-b122c47554f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "    for (number, seqID, dssp, ypred) in list(zip(valid_numbers, valid_dssp_IDs, Y_valid, Y_pred_2)):\n",
    "\n",
    "        # read fasta sequence\n",
    "        fastafile = '/cluster/gjb_lab/2472402/data/retr231/training/' + seqID + '.fasta'\n",
    "        seq = list(SeqIO.parse(open(fastafile),'fasta'))[0] # convert iterator to list then take element 0\n",
    "\n",
    "        # write to SOV file\n",
    "        outfile = out_path + str(number) + '.sov.in'\n",
    "        with open(outfile, 'w') as f:\n",
    "            # write header line\n",
    "            f.write('AA  OSEC PSEC\\n')\n",
    "            # write each line\n",
    "            for (aa, osec_arr, psec_arr) in zip(seq, dssp, ypred):\n",
    "                osec = collapse(osec_arr)\n",
    "                psec = collapse(psec_arr)\n",
    "                f.write(\"%s   %s    %s\\n\" % (aa, osec, psec))\n",
    "\n",
    "    print(\"Finished writing SOV files for cross-val fold %d\\n\" % counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf6a9cc-7200-49b6-91e7-cc7dccbb218b",
   "metadata": {},
   "source": [
    "## Output predictions in SOV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bea441a-d8ce-45e2-92b6-4946f04e62c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import pickle\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b04069-761b-4b52-b2e4-049ba7a55acf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/cluster/gjb_lab/2472402/data/retr231/training/d3kxsa_.fasta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-40c499936755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# read fasta sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfastafile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/cluster/gjb_lab/2472402/data/retr231/training/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseqID\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.fasta'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeqIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfastafile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fasta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# convert iterator to list then take element 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# write to SOV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/cluster/gjb_lab/2472402/data/retr231/training/d3kxsa_.fasta'"
     ]
    }
   ],
   "source": [
    "infiles = [\"/cluster/gjb_lab/2472402/outputs/hmm_cross_val/sov/cv%d/input.pkl\" % num for num in range(1,8)]\n",
    "[valid_numbers, valid_dssp_IDs, Y_valid, Y_pred_2] = pickle.load(open(infiles[0],'rb'))\n",
    "out_path = \"/cluster/gjb_lab/2472402/outputs/hmm_cross_val/sov/tmp/\"\n",
    "\n",
    "for (number, seqID, dssp, ypred) in list(zip(valid_numbers, valid_dssp_IDs, Y_valid, Y_pred_2)):\n",
    "\n",
    "    # read fasta sequence\n",
    "    fastafile = '/cluster/gjb_lab/2472402/data/retr231/training/' + seqID + '.fasta'\n",
    "    seq = list(SeqIO.parse(open(fastafile),'fasta'))[0] # convert iterator to list then take element 0\n",
    "\n",
    "    # write to SOV file\n",
    "    outfile = out_path + str(number) + '.sov.in'\n",
    "    with open(outfile, 'w') as f:\n",
    "        # write header line\n",
    "        f.write('AA  OSEC PSEC\\n')\n",
    "        # write each line\n",
    "        for (aa, osec_arr, psec_arr) in zip(seq, dssp, ypred):\n",
    "            osec = collapse(osec_arr)\n",
    "            psec = collapse(psec_arr)\n",
    "            f.write(\"%s   %s    %s\\n\" % (aa, osec, psec))\n",
    "print(\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a614271-8c2f-46b4-86f9-b9bb4255ccac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Evaluate PSSM layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f85993a-bce6-4cda-8270-49e94a8301cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for cross validation fold 1 are written to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold1_model2_scores.csv\n",
      "Results for cross validation fold 2 are written to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold2_model2_scores.csv\n",
      "Results for cross validation fold 3 are written to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold3_model2_scores.csv\n",
      "Results for cross validation fold 4 are written to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold4_model2_scores.csv\n",
      "Results for cross validation fold 5 are written to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold5_model2_scores.csv\n",
      "Results for cross validation fold 6 are written to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold6_model2_scores.csv\n",
      "Results for cross validation fold 7 are written to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold7_model2_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# first make sure that output path is valid, otherwise computation will go to waste\n",
    "out_path = '/cluster/gjb_lab/2472402/outputs/pssm_cross_val/'\n",
    "assert path.exists(out_path)\n",
    "assert out_path[-1] == '/'\n",
    "\n",
    "\n",
    "# path to training and validation examples\n",
    "paths = [\"/homes/adrozdetskiy/Projects/JnetDatasets/Jnet_training_output_v2/cross-val%d/\" % num for num in range(1,8)]\n",
    "\n",
    "# sequence dictionary or sd. need this to retrieve dssp information\n",
    "sd = pickle.load(open('/cluster/gjb_lab/2472402/data/cross-val/cross_val_dict.pkl','rb')) \n",
    "\n",
    "\n",
    "# for each fold in the 7 fold cross validation procedure, do...\n",
    "for counter, p in enumerate(paths):\n",
    "\n",
    "    counter += 1 # start from 1\n",
    "    \n",
    "    valid_path = p + 'valid/'\n",
    "    \n",
    "    # get file names ending with .pssm\n",
    "    valid_files = [f for f in os.listdir(valid_path) if f[-5:]=='.pssm']\n",
    "    \n",
    "    # read profile pssm as numpy arrays\n",
    "    valid_pssm = [np.genfromtxt(fname = valid_path + fn) for fn in valid_files]\n",
    "    \n",
    "    # window profile pssms to get patterns\n",
    "    # layer 1 sliding window flank = 8\n",
    "    X_valid = [sliding_window(pssm, flank=8) for pssm in valid_pssm]\n",
    "    del valid_pssm\n",
    "    \n",
    "    # get DSSP information \n",
    "    \n",
    "    # remove .pssm extension and convert to int for lookup\n",
    "    valid_numbers = [int(fn[:-5]) for fn in valid_files] \n",
    "    \n",
    "    # lookup seqIDs given jnet number\n",
    "    valid_dssp_IDs = [sd[sd.number == num].letters.values[0] for num in valid_numbers]\n",
    "    \n",
    "    # obtain 3-column DSSP of domains given their seqID. See get_dssp() defined above\n",
    "    Y_valid = [get_dssp(ID) for ID in valid_dssp_IDs]\n",
    "\n",
    "    assert all([y.shape[1] == 3 for y in Y_valid])\n",
    "    assert all([x.shape[0] == y.shape[0] for (x, y) in zip(X_valid, Y_valid)])\n",
    "    \n",
    "    # generate layer 1 predictions\n",
    "    \n",
    "    model1 = keras.models.load_model('/cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold%d_model1' % counter)\n",
    "\n",
    "    Y_pred_1 = [argmax(model1.predict(X)) for X in X_valid]\n",
    "    \n",
    "    # convert layer 1 predictions into layer 2 input\n",
    "    X_valid_2 = [sliding_window(Y, flank = 9) for Y in Y_pred_1]\n",
    "    \n",
    "    # discard model object (and its input) from RAM\n",
    "    del model1\n",
    "    del Y_pred_1\n",
    "    \n",
    "    # sanity checks before passing into model2\n",
    "    assert all([X.shape[1] == 57 for X in X_valid_2])\n",
    "    assert all([X.dtype == 'float64' for X in X_valid_2])\n",
    "    \n",
    "    # generate layer 2 predictions\n",
    "    \n",
    "    model2 = keras.models.load_model('/cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold%d_model2' % counter)\n",
    "\n",
    "    Y_pred_2 = [model2.predict(X) for X in X_valid_2]\n",
    "    \n",
    "    # discard model object (and its input) from RAM\n",
    "    del model2\n",
    "    del X_valid_2\n",
    "    \n",
    "    # open file connection\n",
    "    out_file = out_path + 'fold%d' % counter + '_model2_scores.csv'\n",
    "    with open(out_file, 'w') as f:\n",
    "        # write header line\n",
    "        f.write('seqID,Hcorrect,Htotal,Ecorrect,Etotal,Ccorrect,Ctotal,Ncorrect,Ntotal\\n')\n",
    "\n",
    "        # calculate accuracy using predictions and ground truth \n",
    "        for (num, pred, truth) in zip(valid_numbers, Y_pred_2, Y_valid):\n",
    "            res_arr = get_Qall(pred,truth)\n",
    "            f.write(str(num) + ',')\n",
    "            res_arr.tofile(f, sep=',')\n",
    "            f.write('\\n')\n",
    "    \n",
    "    print('Results for cross validation fold %d are written to %s' % (counter, out_file))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d23efca-b891-44c4-8354-872679a7c048",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Evaluate HMM layer 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df68679f-3e53-46e2-a5a4-5ec5b98ed7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for cross validation fold 1 are written to /cluster/gjb_lab/2472402/outputs/hmm_cross_val/fold1_model1_scores.csv\n",
      "Results for cross validation fold 2 are written to /cluster/gjb_lab/2472402/outputs/hmm_cross_val/fold2_model1_scores.csv\n",
      "Results for cross validation fold 3 are written to /cluster/gjb_lab/2472402/outputs/hmm_cross_val/fold3_model1_scores.csv\n",
      "Results for cross validation fold 4 are written to /cluster/gjb_lab/2472402/outputs/hmm_cross_val/fold4_model1_scores.csv\n",
      "Results for cross validation fold 5 are written to /cluster/gjb_lab/2472402/outputs/hmm_cross_val/fold5_model1_scores.csv\n",
      "Results for cross validation fold 6 are written to /cluster/gjb_lab/2472402/outputs/hmm_cross_val/fold6_model1_scores.csv\n",
      "Results for cross validation fold 7 are written to /cluster/gjb_lab/2472402/outputs/hmm_cross_val/fold7_model1_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# first make sure that output path is valid, otherwise computation will go to waste\n",
    "out_path = '/cluster/gjb_lab/2472402/outputs/hmm_cross_val/'\n",
    "assert path.exists(out_path)\n",
    "assert out_path[-1] == '/'\n",
    "\n",
    "\n",
    "# path to training and validation examples\n",
    "paths = [\"/homes/adrozdetskiy/Projects/JnetDatasets/Jnet_training_output_v2/cross-val%d/\" % num for num in range(1,8)]\n",
    "\n",
    "# sequence dictionary or sd. need this to retrieve dssp information\n",
    "sd = pickle.load(open('/cluster/gjb_lab/2472402/data/cross-val/cross_val_dict.pkl','rb')) \n",
    "\n",
    "\n",
    "# for each fold in the 7 fold cross validation procedure, do...\n",
    "for counter, p in enumerate(paths):\n",
    "\n",
    "    counter += 1 # start from 1\n",
    "    \n",
    "    valid_path = p + 'valid/'\n",
    "    \n",
    "    # get file names ending with .hmm\n",
    "    valid_files = [f for f in os.listdir(valid_path) if f[-4:]=='.hmm']\n",
    "    \n",
    "    # read profile hmms as numpy arrays\n",
    "    valid_hmm = [np.genfromtxt(fname = valid_path + fn) for fn in valid_files]\n",
    "    \n",
    "    # window profile hmms to get patterns\n",
    "    # layer 1 sliding window flank = 8\n",
    "    X_valid = [sliding_window(hmm, flank=8) for hmm in valid_hmm]\n",
    "    del valid_hmm\n",
    "    \n",
    "    # get DSSP information \n",
    "    \n",
    "    # remove .hmm extension and convert to int for lookup\n",
    "    valid_numbers = [int(fn[:-4]) for fn in valid_files] \n",
    "    \n",
    "    # lookup seqIDs given jnet number\n",
    "    valid_dssp_IDs = [sd[sd.number == num].letters.values[0] for num in valid_numbers]\n",
    "    \n",
    "    # obtain 3-column DSSP of domains given their seqID. See get_dssp() defined above\n",
    "    Y_valid = [get_dssp(ID) for ID in valid_dssp_IDs]\n",
    "\n",
    "    assert all([y.shape[1] == 3 for y in Y_valid])\n",
    "    assert all([x.shape[0] == y.shape[0] for (x, y) in zip(X_valid, Y_valid)])\n",
    "    \n",
    "    # generate layer 1 predictions\n",
    "    \n",
    "    model1 = keras.models.load_model('/cluster/gjb_lab/2472402/outputs/hmm_cross_val/fold%d_model1' % counter)\n",
    "\n",
    "    Y_pred_1 = [argmax(model1.predict(X)) for X in X_valid]\n",
    "        \n",
    "    # open file connection\n",
    "    out_file = out_path + 'fold%d' % counter + '_model1_scores.csv'\n",
    "    with open(out_file, 'w') as f:\n",
    "        # write header line\n",
    "        f.write('seqID,Hcorrect,Htotal,Ecorrect,Etotal,Ccorrect,Ctotal,Ncorrect,Ntotal\\n')\n",
    "\n",
    "        # calculate accuracy using predictions and ground truth \n",
    "        for (num, pred, truth) in zip(valid_numbers, Y_pred_1, Y_valid):\n",
    "            res_arr = get_Qall(pred,truth)\n",
    "            f.write(str(num) + ',')\n",
    "            res_arr.tofile(f, sep=',')\n",
    "            f.write('\\n')\n",
    "    \n",
    "    print('Results for cross validation fold %d are written to %s' % (counter, out_file))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d80800-afa7-442d-8c71-15fadf9e34fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Evaluate PSSM layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f02a9f9-8dcb-4cf3-875e-927ba9ea380a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for cross validation fold 1 are written to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold1_model1_scores.csv\n",
      "Results for cross validation fold 2 are written to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold2_model1_scores.csv\n",
      "Results for cross validation fold 3 are written to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold3_model1_scores.csv\n",
      "Results for cross validation fold 4 are written to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold4_model1_scores.csv\n",
      "Results for cross validation fold 5 are written to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold5_model1_scores.csv\n",
      "Results for cross validation fold 6 are written to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold6_model1_scores.csv\n",
      "Results for cross validation fold 7 are written to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold7_model1_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# first make sure that output path is valid, otherwise computation will go to waste\n",
    "out_path = '/cluster/gjb_lab/2472402/outputs/pssm_cross_val/'\n",
    "assert path.exists(out_path)\n",
    "assert out_path[-1] == '/'\n",
    "\n",
    "\n",
    "# path to training and validation examples\n",
    "paths = [\"/homes/adrozdetskiy/Projects/JnetDatasets/Jnet_training_output_v2/cross-val%d/\" % num for num in range(1,8)]\n",
    "\n",
    "# sequence dictionary or sd. need this to retrieve dssp information\n",
    "sd = pickle.load(open('/cluster/gjb_lab/2472402/data/cross-val/cross_val_dict.pkl','rb')) \n",
    "\n",
    "\n",
    "# for each fold in the 7 fold cross validation procedure, do...\n",
    "for counter, p in enumerate(paths):\n",
    "\n",
    "    counter += 1 # start from 1\n",
    "    \n",
    "    valid_path = p + 'valid/'\n",
    "    \n",
    "    # get file names ending with .pssm\n",
    "    valid_files = [f for f in os.listdir(valid_path) if f[-5:]=='.pssm']\n",
    "    \n",
    "    # read profile pssm as numpy arrays\n",
    "    valid_pssm = [np.genfromtxt(fname = valid_path + fn) for fn in valid_files]\n",
    "    \n",
    "    # window profile pssms to get patterns\n",
    "    # layer 1 sliding window flank = 8\n",
    "    X_valid = [sliding_window(pssm, flank=8) for pssm in valid_pssm]\n",
    "    del valid_pssm\n",
    "    \n",
    "    # get DSSP information \n",
    "    \n",
    "    # remove .pssm extension and convert to int for lookup\n",
    "    valid_numbers = [int(fn[:-5]) for fn in valid_files] \n",
    "    \n",
    "    # lookup seqIDs given jnet number\n",
    "    valid_dssp_IDs = [sd[sd.number == num].letters.values[0] for num in valid_numbers]\n",
    "    \n",
    "    # obtain 3-column DSSP of domains given their seqID. See get_dssp() defined above\n",
    "    Y_valid = [get_dssp(ID) for ID in valid_dssp_IDs]\n",
    "\n",
    "    assert all([y.shape[1] == 3 for y in Y_valid])\n",
    "    assert all([x.shape[0] == y.shape[0] for (x, y) in zip(X_valid, Y_valid)])\n",
    "    \n",
    "    # generate layer 1 predictions\n",
    "    \n",
    "    model1 = keras.models.load_model('/cluster/gjb_lab/2472402/outputs/pssm_cross_val/fold%d_model1' % counter)\n",
    "\n",
    "    Y_pred_1 = [argmax(model1.predict(X)) for X in X_valid]\n",
    "        \n",
    "    # open file connection\n",
    "    out_file = out_path + 'fold%d' % counter + '_model1_scores.csv'\n",
    "    with open(out_file, 'w') as f:\n",
    "        # write header line\n",
    "        f.write('seqID,Hcorrect,Htotal,Ecorrect,Etotal,Ccorrect,Ctotal,Ncorrect,Ntotal\\n')\n",
    "\n",
    "        # calculate accuracy using predictions and ground truth \n",
    "        for (num, pred, truth) in zip(valid_numbers, Y_pred_1, Y_valid):\n",
    "            res_arr = get_Qall(pred,truth)\n",
    "            f.write(str(num) + ',')\n",
    "            res_arr.tofile(f, sep=',')\n",
    "            f.write('\\n')\n",
    "    \n",
    "    print('Results for cross validation fold %d are written to %s' % (counter, out_file))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

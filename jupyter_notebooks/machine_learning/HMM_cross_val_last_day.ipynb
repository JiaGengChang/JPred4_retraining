{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563270a6-2474-4741-bff7-decbf1ffd764",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HMM cross validation notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf4285d-7c53-41b1-9eec-5312d51c449b",
   "metadata": {},
   "source": [
    "Last ran on: 20/09/2021\n",
    "\n",
    "This version runs on serialized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "52c17dcf-5e7e-4a71-9e7d-ed7fa9d491cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/cluster/gjb_lab/2472402/envs/ml-env/bin/python\n",
    "#$ -pe smp 16\n",
    "#$ -jc short\n",
    "#$ -adds l_hard gpu 4\n",
    "#$ -mods l_hard mfree 16G\n",
    "#$ -m ea\n",
    "#$ -M 2472402@dundee.ac.uk\n",
    "#$ -wd /cluster/gjb_lab/2472402/outputs/2021-09-03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ee89a-ea78-40cf-bdcc-3763f62fe90c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57218031-1832-4192-b5e2-c642ad908e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from os import path\n",
    "from joblib import Parallel,delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9fbe2-ac1b-4852-b9ff-c12a72b965f6",
   "metadata": {},
   "source": [
    "# Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca0da6b-ee79-40a2-8e89-cca4411fd564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is copied from HMM.ipynb\n",
    "# array: numpy array\n",
    "# flank: positive integer\n",
    "def sliding_window(array, flank):\n",
    "    assert flank > 0\n",
    "    assert type(array) is np.ndarray\n",
    "    assert np.logical_not(np.isnan(np.sum(array)))\n",
    "    nrow = array.shape[0]\n",
    "    assert nrow > 0\n",
    "    ncol = array.shape[1]\n",
    "    assert ncol > 0\n",
    "    res = np.empty(shape=(nrow, (2*flank+1)*ncol))\n",
    "    res[:] = np.nan\n",
    "    for i in list(range(0,nrow)):\n",
    "        s, e = i-flank, i+flank+1\n",
    "        k = 0;\n",
    "        for j in list(range(s,e)):\n",
    "            if (j < 0 or j >= nrow):\n",
    "                res[i, k:k+ncol] = 0\n",
    "            else:\n",
    "                assert np.logical_not(np.isnan(np.sum(array[j])))\n",
    "                assert array[j].shape == (ncol,)\n",
    "                res[i, k:k+ncol] = array[j]\n",
    "            k += ncol\n",
    "    assert np.logical_not(np.isnan(np.sum(res)))\n",
    "    assert res.shape == (nrow, (2*flank+1)*ncol)\n",
    "    return res\n",
    "\n",
    "# this function rounds predictions into 1 and 0s\n",
    "def argmax(arr):\n",
    "    n, c = arr.shape\n",
    "    assert c == 3\n",
    "    assert type(arr) is np.ndarray\n",
    "    assert np.logical_not(np.isnan(np.sum(arr)))\n",
    "    res = np.empty(shape=(n,c))\n",
    "    res[:] = np.nan\n",
    "    for i in list(range(0,n)):\n",
    "        max_idx = np.argmax(arr[i])\n",
    "        if max_idx == 0:\n",
    "            res[i] = np.array([1, 0, 0])\n",
    "        elif max_idx == 1:\n",
    "            res[i] = np.array([0, 1, 0])\n",
    "        else:\n",
    "            assert max_idx == 2\n",
    "            res[i] = np.array([0, 0, 1])\n",
    "    assert np.logical_not(np.isnan(np.sum(res)))\n",
    "    return res\n",
    "\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)\n",
    "    \n",
    "# given string of dssp s, one hot encode it in E, H,C order ## note this is different from previous ways\n",
    "# e.g. input\n",
    "def onehotstring(s):\n",
    "    res = np.empty(shape=(len(s),3))\n",
    "    res[:] = np.nan\n",
    "    for i in range(0,len(s)):\n",
    "        if s[i] == 'H':\n",
    "            res[i] = np.array([0,1,0])\n",
    "        else:\n",
    "            if s[i] == 'E':\n",
    "                res[i] = np.array([1,0,0])\n",
    "            else:\n",
    "                assert s[i]\n",
    "                res[i] = np.array([0,0,1])\n",
    "    assert not np.isnan(np.sum(res))\n",
    "    return res\n",
    "\n",
    "# obtain 7 sets of indices from resume.log, which is an output of shuffle.pl\n",
    "def get_splits(resume_log_file):\n",
    "    val_splits = []\n",
    "    set_idx = -1\n",
    "    cur_set = set() \n",
    "    with open(resume_log_file,'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        for line in lines:\n",
    "            if line.startswith('#SET'):\n",
    "                if set_idx > -1:\n",
    "                    val_splits.append(cur_set)\n",
    "                    cur_set = set()\n",
    "                set_idx += 1\n",
    "            else:\n",
    "                seqID = line.split('/')[-1].replace('.pssm','')\n",
    "                cur_set.add(seqID)\n",
    "        # append last set which is not followed by another line '#SET...'\n",
    "        val_splits.append(cur_set)\n",
    "    assert sum([len(s) for s in val_splits])==1348\n",
    "    return val_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f55bc8-1148-4a1f-a092-eb4721360893",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cross-validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "032b2b37-e6a6-4e0a-bb40-45c755158389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_CV(debug=False,**args):\n",
    "    \n",
    "    subtype = args['subtype']\n",
    "    BATCH_SIZE = args['BATCH_SIZE']\n",
    "    N_HID = args['N_HID']\n",
    "    N_EPOCHS = args['N_EPOCHS']\n",
    "    expt_name = args['expt_name']\n",
    "\n",
    "    assert subtype in ['HMM','PSSMa','PSSMb']\n",
    "    assert expt_name\n",
    "    \n",
    "    kf = get_splits('/cluster/gjb_lab/2472402/data/retr231_shuffles/shuffle02/best_shuffle_th_1.log')\n",
    "    \n",
    "    ROOT='/cluster/gjb_lab/2472402/data/retr231/'\n",
    "    subtype_root = {'HMM':'hmm','PSSMa':'pssm','PSSMb':'pssm'}[subtype]\n",
    "    DATA=ROOT + subtype_root + '1.pkl'\n",
    "    DSSP=ROOT+'dssp.pkl'    \n",
    "    \n",
    "    with open(DATA,'rb') as f:\n",
    "        X=pickle.load(f)\n",
    "    with open(DSSP,'rb') as f:\n",
    "        y=pd.DataFrame(pickle.load(f))\n",
    "    \n",
    "    X.loc[:,'seqID'] = [c.split('_')[0] for c in X.index]\n",
    "    y.loc[:,'seqID'] = [c.split('_')[0] for c in y.index]    \n",
    "    \n",
    "    for k in range(7):\n",
    "\n",
    "        print('Beginning cross validation fold',k+1)\n",
    "        \n",
    "        # gather train and validation seqIDs\n",
    "        valid_idx = list(kf[k])\n",
    "        train_idx = (set().union(*(kf[0:k] + kf[k+1:])))\n",
    "        # partition train and validation data\n",
    "        X_train = X[X['seqID'].isin(train_idx)].drop(['seqID'],axis=1)\n",
    "        X_valid = X[X['seqID'].isin(valid_idx)].drop(['seqID'],axis=1)\n",
    "        y_train = y[y['seqID'].isin(train_idx)].drop(['seqID'],axis=1)\n",
    "        y_valid = y[y['seqID'].isin(valid_idx)].drop(['seqID'],axis=1)\n",
    "        \n",
    "        # create output folder\n",
    "        i = k+1\n",
    "        subtype_dir = {'HMM':'HMM','PSSMa':'PSSM','PSSMb':'PSSMb'}[subtype]\n",
    "        root_folder=\"/cluster/gjb_lab/2472402/outputs/keras_train_CV/%s/%s\" % (subtype_dir,expt_name)\n",
    "        assert path.exists(root_folder), \"Root folder %s does not exist\" % root_folder\n",
    "        # create out folder\n",
    "        out_folder=os.path.join(root_folder, \"cross-val%d/\" % i)\n",
    "        if path.exists(out_folder) and not debug:\n",
    "            assert not os.listdir(out_folder), \"Output directory %s exists and is not empty. Aborted.\" % out_folder\n",
    "        else:\n",
    "            if not path.exists(out_folder):\n",
    "                os.system(\"mkdir %s\" % out_folder)\n",
    "            else:\n",
    "                eprint(out_folder,'already exists and in debugging mode so using that.')\n",
    "\n",
    "        if debug:\n",
    "            X_train = X_train[::100]\n",
    "            X_valid = X_valid[::100]\n",
    "            y_train = y_train[::100]\n",
    "            y_valid = y_valid[::100]\n",
    "\n",
    "        if debug and os.path.exists(out_folder+'/model1'):\n",
    "            eprint('In debugging mode and model1 found. Loading model1...')\n",
    "            model1 = keras.models.load_model(out_folder+'/model1')\n",
    "        else:\n",
    "            # sequence to structure layer\n",
    "            model1 = keras.Sequential([\n",
    "                layers.Dense(units = N_HID, activation ='sigmoid', input_shape=[408 if subtype=='HMM' else 340]),\n",
    "                layers.Dense(units = 3, activation ='softmax')\n",
    "            ])\n",
    "\n",
    "            model1.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "            eprint('%s. Fitting layer 1 model.' % datetime.now().strftime(\"%H:%M:%S\"))\n",
    "            history1 = model1.fit(X_train, y_train,\n",
    "                                  validation_data=(X_valid,y_valid),\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  epochs=N_EPOCHS,\n",
    "                                  verbose=0)\n",
    "\n",
    "            eprint('%s. Saving model 1...' % datetime.now().strftime(\"%H:%M:%S\"))\n",
    "            model1_folder = out_folder + 'model1'\n",
    "            model1.save(model1_folder,save_format = 'tf') # tensorflow SavedModel format\n",
    "        \n",
    "        eprint('Processing layer 1 inputs for layer 2')\n",
    "        # structure to structure layer\n",
    "        train_group = X[X['seqID'].isin(train_idx)].groupby(['seqID'])\n",
    "        valid_group = X[X['seqID'].isin(valid_idx)].groupby(['seqID'])\n",
    "        Xtrain1_list = [groupby_df.drop(['seqID'],axis=1) for _,groupby_df in train_group]\n",
    "        Xvalid1_list = [groupby_df.drop(['seqID'],axis=1) for _,groupby_df in valid_group]\n",
    "        \n",
    "        yvalid_group = y[y['seqID'].isin(valid_idx)].groupby(['seqID'])\n",
    "        Yvalid_list= [groupby_df.drop(['seqID'],axis=1) for _,groupby_df in yvalid_group]\n",
    "        valid_seqIDs = [groupby_df['seqID'][0] for _,groupby_df in yvalid_group]\n",
    "        \n",
    "        assert all([isinstance(seqID,str) for seqID in valid_seqIDs])\n",
    "        \n",
    "        del valid_group, train_group, yvalid_group\n",
    "\n",
    "        # skin the cat - pass layer 1 input through model 1 to get layer 2 input\n",
    "        Xtrain2_list = [sliding_window(model1.predict(X),flank=9) for X in Xtrain1_list]\n",
    "        Yvalid_pred1 = [model1.predict(X) for X in Xvalid1_list]\n",
    "        Xvalid2_list = [sliding_window(X,flank=9) for X in Yvalid_pred1]\n",
    "\n",
    "        Xtrain2=np.vstack(Xtrain2_list)\n",
    "        Xvalid2=np.vstack(Xvalid2_list)\n",
    "\n",
    "        if debug:\n",
    "            Xtrain2 = Xtrain2[::100]\n",
    "            Xvalid2 = Xvalid2[::100]\n",
    "        \n",
    "        if debug and os.path.exists(out_folder+'/model2'):\n",
    "            eprint('In debugging mode and model2 found. Loading model2...')\n",
    "            model2 = keras.models.load_model(out_folder+'/model2')\n",
    "        else:\n",
    "            # structure to structure layer\n",
    "            model2 = keras.Sequential([\n",
    "                layers.Dense(units = N_HID, activation ='sigmoid', input_shape=[57]),\n",
    "                layers.Dense(units = 3, activation ='softmax')\n",
    "            ])\n",
    "            model2.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "            eprint('%s. Fitting layer 2 model.' % datetime.now().strftime(\"%H:%M:%S\"))\n",
    "            history2 = model2.fit(Xtrain2, y_train,\n",
    "                                  validation_data=(Xvalid2,y_valid),\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  epochs=N_EPOCHS,\n",
    "                                  verbose=0) \n",
    "\n",
    "            eprint('%s. Saving layer 2 model.' % datetime.now().strftime(\"%H:%M:%S\"))\n",
    "            model2_folder = out_folder + 'model2'\n",
    "            model2.save(model2_folder,save_format = 'tf') # tensorflow SavedModel format\n",
    "        \n",
    "            # free up RAM\n",
    "            del Xtrain2, Xvalid2\n",
    "\n",
    "        # get predictions for validation set (to calculate accuracy)\n",
    "        eprint('%s. Generating layer 2 predictions.' % datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        Yvalid_pred2=[model2.predict(X) for X in Xvalid2_list]\n",
    "\n",
    "        # pickle training history and predictions for validation set \n",
    "        eprint('%s. Saving model history.' % datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        results_file = out_folder + 'results.pkl'\n",
    "        \n",
    "        if debug:\n",
    "            eprint('%s. Skipped saving of model history.' % datetime.now().strftime(\"%H:%M:%S\"))\n",
    "            with open(results_file,'wb') as f:\n",
    "                pickle.dump(obj=((None,None), # histories for plotting\n",
    "                                 (valid_seqIDs,Yvalid_list,Yvalid_pred1,Yvalid_pred2)), # domains with their truths and predictions\n",
    "                            file=f,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)                \n",
    "        else:\n",
    "            with open(results_file,'wb') as f:\n",
    "                pickle.dump(obj=((history1.history, history2.history), # histories for plotting\n",
    "                                 (valid_seqIDs,Yvalid_list,Yvalid_pred1,Yvalid_pred2)), # domains with their truths and predictions\n",
    "                            file=f,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e483a67-16e2-4187-a916-57c2f793096c",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4439b6ef-4cd7-44f6-bd62-f30d4fc955a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running in debugging mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning cross validation fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/gjb_lab/2472402/outputs/keras_train_CV/HMM/16Sep/cross-val1/ already exists and in debugging mode so using that.\n",
      "In debugging mode and model1 found. Loading model1...\n",
      "2021-09-20 23:09:25.948915: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "Processing layer 1 inputs for layer 2\n",
      "In debugging mode and model2 found. Loading model2...\n",
      "23:11:51. Generating layer 2 predictions.\n",
      "23:11:59. Saving model history.\n",
      "23:11:59. Skipped saving of model history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning cross validation fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/gjb_lab/2472402/outputs/keras_train_CV/HMM/16Sep/cross-val2/ already exists and in debugging mode so using that.\n",
      "In debugging mode and model1 found. Loading model1...\n",
      "Processing layer 1 inputs for layer 2\n",
      "In debugging mode and model2 found. Loading model2...\n",
      "23:14:15. Generating layer 2 predictions.\n",
      "23:14:25. Saving model history.\n",
      "23:14:25. Skipped saving of model history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning cross validation fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/gjb_lab/2472402/outputs/keras_train_CV/HMM/16Sep/cross-val3/ already exists and in debugging mode so using that.\n",
      "In debugging mode and model1 found. Loading model1...\n",
      "Processing layer 1 inputs for layer 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/234748.1.all.q/ipykernel_43504/66230844.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     }\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mrun_CV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mhmm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mrun_CV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpssma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mrun_CV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpssmb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/234748.1.all.q/ipykernel_43504/836026261.py\u001b[0m in \u001b[0;36mrun_CV\u001b[0;34m(debug, **args)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# skin the cat - pass layer 1 input through model 1 to get layer 2 input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mXtrain2_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msliding_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mXtrain1_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mYvalid_pred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mXvalid1_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mXvalid2_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msliding_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mYvalid_pred1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/234748.1.all.q/ipykernel_43504/836026261.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# skin the cat - pass layer 1 input through model 1 to get layer 2 input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mXtrain2_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msliding_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mXtrain1_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mYvalid_pred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mXvalid1_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mXvalid2_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msliding_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mYvalid_pred1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1721\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1724\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    694\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    717\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 719\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3118\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3119\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3120\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3121\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3122\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    DEBUG=1\n",
    "    \n",
    "    if DEBUG:\n",
    "        eprint('Running in debugging mode')\n",
    "    \n",
    "    expt_name = '16Sep'\n",
    "    N_EPOCHS = 2\n",
    "\n",
    "    hmm = {\n",
    "        'subtype':'HMM',\n",
    "        'BATCH_SIZE':128,\n",
    "        'N_HID':100,\n",
    "        'N_EPOCHS':N_EPOCHS,\n",
    "        'expt_name':expt_name\n",
    "    }\n",
    "    pssma = {\n",
    "        'subtype':'PSSMa',\n",
    "        'BATCH_SIZE':128,\n",
    "        'N_HID':100,\n",
    "        'N_EPOCHS':N_EPOCHS,\n",
    "        'expt_name':expt_name\n",
    "    }  \n",
    "    pssmb = {\n",
    "        'subtype':'PSSMb',\n",
    "        'BATCH_SIZE':128,\n",
    "        'N_HID':20,\n",
    "        'N_EPOCHS':N_EPOCHS,\n",
    "        'expt_name':expt_name\n",
    "    }\n",
    "        \n",
    "    run_CV(debug=DEBUG,**hmm)\n",
    "    run_CV(debug=DEBUG,**pssma)\n",
    "    run_CV(debug=DEBUG,**pssmb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ml-env)",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

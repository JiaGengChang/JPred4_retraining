{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563270a6-2474-4741-bff7-decbf1ffd764",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HMM cross validation notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf4285d-7c53-41b1-9eec-5312d51c449b",
   "metadata": {},
   "source": [
    "Script partially adapted from cross_val_old.ipynb. A similar version of this script, called PSSM_cross_val.ipynb, exists\n",
    "\n",
    "This notebook runs on hmm/pssm files using the sets defined in archives available online. The datasets are obtained from /homes/adrozdetskiy/Projects/jpredJnet231ReTrainingSummaryTable/data/training/(1348)\n",
    "\n",
    "I created this notebook to run checks so that the equivalent .py script will not crash when submitted to the scheduler\n",
    "\n",
    "The actual script used to run cross validation is stored in /cluster/gjb_lab/2472402/scripts/hmm_cross_val.py, which is wrapped by a bash script (hmm_cross_val.sh) for submission to the scheduler\n",
    "\n",
    "Last changes made: 24 Aug 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca0da6b-ee79-40a2-8e89-cca4411fd564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is copied from HMM.ipynb\n",
    "# array: numpy array\n",
    "# flank: positive integer\n",
    "def sliding_window(array, flank):\n",
    "    assert flank > 0\n",
    "    assert type(array) is np.ndarray\n",
    "    assert np.logical_not(np.isnan(np.sum(array)))\n",
    "    nrow = array.shape[0]\n",
    "    assert nrow > 0\n",
    "    ncol = array.shape[1]\n",
    "    assert ncol > 0\n",
    "    res = np.empty(shape=(nrow, (2*flank+1)*ncol))\n",
    "    res[:] = np.nan\n",
    "    for i in list(range(0,nrow)):\n",
    "        s, e = i-flank, i+flank+1\n",
    "        k = 0;\n",
    "        for j in list(range(s,e)):\n",
    "            if (j < 0 or j >= nrow):\n",
    "                res[i, k:k+ncol] = 0\n",
    "            else:\n",
    "                assert np.logical_not(np.isnan(np.sum(array[j])))\n",
    "                assert array[j].shape == (ncol,)\n",
    "                res[i, k:k+ncol] = array[j]\n",
    "            k += ncol\n",
    "    assert np.logical_not(np.isnan(np.sum(res)))\n",
    "    assert res.shape == (nrow, (2*flank+1)*ncol)\n",
    "    return res\n",
    "\n",
    "# this function rounds predictions into 1 and 0s\n",
    "def argmax(arr):\n",
    "    n, c = arr.shape\n",
    "    assert c == 3\n",
    "    assert type(arr) is np.ndarray\n",
    "    assert np.logical_not(np.isnan(np.sum(arr)))\n",
    "    res = np.empty(shape=(n,c))\n",
    "    res[:] = np.nan\n",
    "    for i in list(range(0,n)):\n",
    "        max_idx = np.argmax(arr[i])\n",
    "        if max_idx == 0:\n",
    "            res[i] = np.array([1, 0, 0])\n",
    "        elif max_idx == 1:\n",
    "            res[i] = np.array([0, 1, 0])\n",
    "        else:\n",
    "            assert max_idx == 2\n",
    "            res[i] = np.array([0, 0, 1])\n",
    "    assert np.logical_not(np.isnan(np.sum(res)))\n",
    "    return res\n",
    "\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)\n",
    "    \n",
    "# given string of dssp s, one hot encode it in E, H,C order ## note this is different from previous ways\n",
    "# e.g. input\n",
    "def onehotstring(s):\n",
    "    res = np.empty(shape=(len(s),3))\n",
    "    res[:] = np.nan\n",
    "    for i in range(0,len(s)):\n",
    "        if s[i] == 'H':\n",
    "            res[i] = np.array([0,1,0])\n",
    "        else:\n",
    "            if s[i] == 'E':\n",
    "                res[i] = np.array([1,0,0])\n",
    "            else:\n",
    "                assert s[i]\n",
    "                res[i] = np.array([0,0,1])\n",
    "    assert not np.isnan(np.sum(res))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65518f3-9fb2-427f-a684-29ba44615a8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Wrapper script to run cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "032b2b37-e6a6-4e0a-bb40-45c755158389",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train size (should be 1155 or 1157): 1155\n",
      "validation size (should be 193 or 191): 193\n",
      "00:15:51. Fitting layer 1 model.\n",
      "00:15:52. Fitting layer 2 model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 17108 calls to <function Model.make_train_function.<locals>.train_function at 0x2b061af6e8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 2787 calls to <function Model.make_test_function.<locals>.test_function at 0x2b061afc58b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:15:52. Generating layer 2 predictions.\n",
      "00:15:52. Saving inputs and results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/keras_train_CV/PSSM/24Aug/cross-val1/model1/assets\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/keras_train_CV/PSSM/24Aug/cross-val1/model2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train size (should be 1155 or 1157): 1155\n",
      "validation size (should be 193 or 191): 193\n",
      "00:16:39. Fitting layer 1 model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 17109 calls to <function Model.make_train_function.<locals>.train_function at 0x2b0623965940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 2788 calls to <function Model.make_test_function.<locals>.test_function at 0x2b061ad4d430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:16:40. Fitting layer 2 model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 17110 calls to <function Model.make_train_function.<locals>.train_function at 0x2b061bc80c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 2789 calls to <function Model.make_test_function.<locals>.test_function at 0x2b061a846c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:16:40. Generating layer 2 predictions.\n",
      "00:16:41. Saving inputs and results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/keras_train_CV/PSSM/24Aug/cross-val2/model1/assets\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/keras_train_CV/PSSM/24Aug/cross-val2/model2/assets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d0b2172b87c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_profile_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mpattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msliding_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mXtrain1_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a373fdef2627>\u001b[0m in \u001b[0;36msliding_window\u001b[0;34m(array, flank)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mncol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mncol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mncol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/miniconda/envs/sandbox/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2247\u001b[0;31m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0m\u001b[1;32m   2248\u001b[0m                           initial=initial, where=where)\n\u001b[1;32m   2249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/miniconda/envs/sandbox/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from os import path\n",
    "\n",
    "subtype='HMM' # <-- change this value\n",
    "\n",
    "assert subtype=='PSSM' or subtype=='HMM'\n",
    "if subtype=='HMM':\n",
    "    ext=\".hmm\"\n",
    "    L1_INPUT_SHAPE=408\n",
    "else:\n",
    "    assert subtype=='PSSM'\n",
    "    ext=\".pssm\"\n",
    "    L1_INPUT_SHAPE=340\n",
    "\n",
    "# declare constants\n",
    "BATCH_SIZE=128 \n",
    "N_EPOCHS=300\n",
    "N_HID=100\n",
    "\n",
    "debug=True\n",
    "# in future use /cluster/homes/... instead of /homes/...\n",
    "all_seqIDs_folder=\"/homes/adrozdetskiy/Projects/jpredJnet231ReTrainingSummaryTable/scores/training/\"\n",
    "all_domains_folder=\"/homes/adrozdetskiy/Projects/jpredJnet231ReTrainingSummaryTable/scores/training/1348/\"\n",
    "\n",
    "for i in range(1,8):\n",
    "    \n",
    "    root_folder=\"/cluster/gjb_lab/2472402/outputs/keras_train_CV/%s/24Aug\" % subtype\n",
    "    assert path.exists(root_folder), \"Root folder %s does not exist\" % root_folder\n",
    "    # create out folder\n",
    "    out_folder=os.path.join(root_folder, \"cross-val%d/\" % i)\n",
    "    if path.exists(out_folder):\n",
    "        assert not os.listdir(out_folder), \"Output directory %s exists and is not empty. Aborted.\" % out_folder\n",
    "    else:\n",
    "        os.system(\"mkdir %s\" % out_folder)\n",
    "    \n",
    "    Xtrain1_list,Ytrain_list=[],[] #Ytrain is same for both layers\n",
    "    Xvalid1_list,Yvalid_list=[],[] #Yvalid is same for both layers\n",
    "    \n",
    "    train_folder=\"/cluster/gjb_lab/2472402/snns_cross_val_24_Aug/cross-val%d/data/\" % i;\n",
    "    valid_folder=\"/cluster/gjb_lab/2472402/snns_cross_val_24_Aug/cross-val%d/valid/\" % i;\n",
    "    train_seqIDs=sorted([_[:-4] for _ in os.listdir(train_folder) if _.endswith('.hmm')])\n",
    "    valid_seqIDs=sorted([_[:-4] for _ in os.listdir(valid_folder) if _.endswith('.hmm')])\n",
    "    assert valid_seqIDs\n",
    "    assert train_seqIDs\n",
    "    \n",
    "    # read the pssm/hmm files\n",
    "    train_profile_files=sorted([_ for _ in os.listdir(train_folder) if _.endswith(ext)])\n",
    "    valid_profile_files=sorted([_ for _ in os.listdir(valid_folder) if _.endswith(ext)])\n",
    "    assert train_profile_files\n",
    "    assert valid_profile_files\n",
    "    \n",
    "    # read the dssp files - both of which are in train_folder (jpred training code bug?)\n",
    "    train_dssp_files=sorted([os.path.join(train_folder,_+'.dssp') for _ in train_seqIDs])\n",
    "    valid_dssp_files=sorted([os.path.join(train_folder,_+'.dssp') for _ in valid_seqIDs])\n",
    "    assert train_dssp_files\n",
    "    assert valid_dssp_files\n",
    "    \n",
    "    # read input files one by one and convert to list of patterns\n",
    "    for f in train_profile_files:\n",
    "        profile=np.genfromtxt(train_folder+f)\n",
    "        pattern=sliding_window(profile,flank=8)\n",
    "        Xtrain1_list.append(pattern)\n",
    "\n",
    "    for f in valid_profile_files:\n",
    "        profile=np.genfromtxt(valid_folder+f)\n",
    "        pattern=sliding_window(profile,flank=8)\n",
    "        Xvalid1_list.append(pattern)\n",
    "    \n",
    "    # read dssp information \n",
    "    for dsspf in train_dssp_files:\n",
    "        with open(dsspf,'r') as f:\n",
    "            string_dssp=f.read().rstrip() # rstrip is perl equivalent of chomp\n",
    "            dssp=onehotstring(string_dssp)\n",
    "            Ytrain_list.append(dssp)\n",
    "    assert Ytrain_list\n",
    "    \n",
    "    for dsspf in valid_dssp_files:\n",
    "        with open(dsspf,'r') as f:\n",
    "            string_dssp=f.read().rstrip()\n",
    "            dssp=onehotstring(string_dssp)\n",
    "            Yvalid_list.append(dssp)\n",
    "    assert Yvalid_list\n",
    "    \n",
    "    # end of part modified 24 Aug\n",
    "    \n",
    "    assert len(Xtrain1_list)==len(Ytrain_list) # should see 1151 or 1153 sequences\n",
    "    eprint(\"train size (should be 1155 or 1157): %d\" % len(Ytrain_list))\n",
    "    assert len(Xvalid1_list)==len(Yvalid_list) # should see 193 or 191 sequences\n",
    "    eprint(\"validation size (should be 193 or 191): %d\" % len(Yvalid_list))\n",
    "    \n",
    "    if (debug):\n",
    "        Xtrain1_list=Xtrain1_list[0:1]\n",
    "        Ytrain_list=Ytrain_list[0:1]\n",
    "        Xvalid1_list=Xvalid1_list[0:1]\n",
    "        Yvalid_list=Yvalid_list[0:1]\n",
    "        N_EPOCHS=1\n",
    "        N_HID=1\n",
    "    \n",
    "    # collapse the domain-level partitioning of the patterns\n",
    "    Xtrain1=np.vstack(Xtrain1_list) \n",
    "    Ytrain=np.vstack(Ytrain_list) \n",
    "    Xvalid1=np.vstack(Xvalid1_list) \n",
    "    Yvalid=np.vstack(Yvalid_list) \n",
    "    assert sum([df.shape[0] for df in Xtrain1_list])==sum([df.shape[0] for df in Ytrain_list])\n",
    "    \n",
    "    if (debug):\n",
    "        Xtrain1=Xtrain1[0:1]\n",
    "        Ytrain=Ytrain[0:1]\n",
    "        Xvalid1=Xvalid1[0:1]\n",
    "        Yvalid=Yvalid[0:1]\n",
    "    \n",
    "    # sequence to structure layer\n",
    "    model1 = keras.Sequential([\n",
    "        layers.Dense(units = N_HID, activation ='sigmoid', input_shape=[L1_INPUT_SHAPE]),\n",
    "        layers.Dense(units = 3, activation ='softmax')\n",
    "    ])\n",
    "    \n",
    "    model1.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
    "    \n",
    "    eprint('%s. Fitting layer 1 model.' % datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    history1 = model1.fit(Xtrain1, Ytrain,\n",
    "                          validation_data=(Xvalid1,Yvalid),\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          epochs=N_EPOCHS,\n",
    "                          verbose=0)    \n",
    "    \n",
    "    # free up RAM\n",
    "    del Xtrain1, Xvalid1\n",
    "    \n",
    "    # skin the cat - pass layer 1 input through model 1 to get layer 2 input\n",
    "    Xtrain2_list = [sliding_window(model1.predict(X),flank=9) for X in Xtrain1_list]\n",
    "    Yvalid_pred1 = [model1.predict(X) for X in Xvalid1_list]\n",
    "    Xvalid2_list = [sliding_window(X,flank=9) for X in Yvalid_pred1]\n",
    "    \n",
    "    Xtrain2=np.vstack(Xtrain2_list)\n",
    "    Xvalid2=np.vstack(Xvalid2_list)\n",
    "    #Yvalid is the same as layer 1\n",
    "    #Ytrain is the same as layer 1\n",
    "    \n",
    "    if (debug):\n",
    "        Xtrain2=Xtrain2[0:1]\n",
    "        Xvalid2=Xvalid2[0:1]\n",
    "    \n",
    "    # structure to structure layer\n",
    "    model2 = keras.Sequential([\n",
    "        layers.Dense(units = N_HID, activation ='sigmoid', input_shape=[57]),\n",
    "        layers.Dense(units = 3, activation ='softmax')\n",
    "    ])\n",
    "    model2.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
    "    \n",
    "    eprint('%s. Fitting layer 2 model.' % datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    history2 = model2.fit(Xtrain2, Ytrain,\n",
    "                          validation_data=(Xvalid2,Yvalid),\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          epochs=N_EPOCHS,\n",
    "                          verbose=0) \n",
    "    \n",
    "    # free up RAM\n",
    "    del Xtrain2, Xvalid2\n",
    "    \n",
    "    # get predictions for validation set (to calculate accuracy)\n",
    "    eprint('%s. Generating layer 2 predictions.' % datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    Yvalid_pred2=[model2.predict(X) for X in Xvalid2_list]\n",
    "    \n",
    "    # pickle training history and predictions for validation set \n",
    "    eprint('%s. Saving inputs and results.' % datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    results_file = out_folder + 'results.pkl'\n",
    "    model1_folder = out_folder + 'model1'\n",
    "    model2_folder = out_folder + 'model2'\n",
    "    # write out the results of the training\n",
    "    with open(results_file,'wb') as f:\n",
    "        pickle.dump(obj=((history1.history, history2.history), # histories for plotting\n",
    "                         (valid_seqIDs,Yvalid_list,Yvalid_pred1,Yvalid_pred2)), # domains with their truths and predictions\n",
    "                    file=f,\n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # save sequence model and structure model\n",
    "    model1.save(model1_folder,save_format = 'tf') # tensorflow SavedModel format\n",
    "    model2.save(model2_folder,save_format = 'tf') # tensorflow SavedModel format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa6a85-4d73-4de0-af21-c41218e3757b",
   "metadata": {},
   "source": [
    "# Create dictionary mapping seqIDs to domains for each cross validation fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072bb4de-eb16-402e-bff8-2f1e281e6b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of horrible valid_dssp_files section\n",
    "# caveat - the valid directory does not have dssp files, so they have to be read from the 1348 folder ...\n",
    "# ...where files are named as domains\n",
    "# the following approach is O(N) complexity\n",
    "# Using valid_seqIDs to create a variable called valid_dssp_domains...\n",
    "# create dictionary to convert between seqID and domain\n",
    "all_seqIDs=sorted([_[:-4] for _ in os.listdir(all_seqIDs_folder)if _.endswith('.hmm')])    \n",
    "all_domains=sorted([_[:-4] for _ in os.listdir(all_domains_folder)if _.endswith('.hmm')])    \n",
    "seqID_domain_dict = dict()\n",
    "for (seqID,dom) in zip(all_seqIDs,all_domains):\n",
    "    assert seqID not in seqID_domain_dict\n",
    "    seqID_domain_dict[seqID]=dom\n",
    "assert seqID_domain_dict\n",
    "# obtain names of domains from validation set\n",
    "valid_domains=[]\n",
    "for seqID in valid_seqIDs:\n",
    "    assert seqID in all_seqIDs\n",
    "    valid_domains.append(seqID_domain_dict[seqID])\n",
    "assert valid_domains\n",
    "assert len(valid_domains)==len(valid_seqIDs)\n",
    "# obtain the desired 191 or 193 dssp files from the 1348 folder, which contains 1348 dssp files\n",
    "valid_dssp_files=[all_domains_folder+_+'.dssp' for _ in valid_domains]\n",
    "\n",
    "assert train_dssp_files\n",
    "assert valid_dssp_files\n",
    "# end of horrible valid_dssp_files section\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02eb6403-bb8a-4907-912b-6ef0408ee51e",
   "metadata": {},
   "source": [
    "# Cross validation from serialized dataset\n",
    "\n",
    "> Faster than HMM_cross_val.py and PSSM_cross_val.py because it does not require preprocessing functions\n",
    "\n",
    "> I am using sklearn here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9cda78-53ee-4fc5-a4f3-2e2e532f5c94",
   "metadata": {},
   "source": [
    "Qsub parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a53f914-4aa2-4c50-ac82-649fc3a1c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$ -pe smp 16\n",
    "#$ -jc short\n",
    "#$ -adds l_hard gpu 4\n",
    "#$ -mods l_hard mfree 16G\n",
    "#$ -m ea\n",
    "#$ -M 2472402@dundee.ac.uk\n",
    "#$ -wd /cluster/gjb_lab/2472402/outputs/2021-09-03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761aa562-e1ab-4baa-9431-278c378842eb",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1c66daab-5cdc-4bc2-adfd-abcc785cbae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import neural_network\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de7f94-f954-4fbd-aaf2-f3a4e026f19d",
   "metadata": {},
   "source": [
    "Load serialized datasets\n",
    "> Serialized datasets are obtained from SNNS hmm1.pat and pssm1.pat pattern files\n",
    "\n",
    "> These pattern files are in turn obtained from running train_network.pl (commenting out the last line train_net())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "178d41ff-10fa-429a-9aa4-07d42f32b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA='/cluster/gjb_lab/2472402/data/retr231/'\n",
    "HMM=DATA+'hmm1.pkl'\n",
    "PSSM=DATA+'pssm1.pkl'\n",
    "DSSP=DATA+'dssp-vec.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9b0af677-3349-43b4-9154-b2808e8358a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HMM,'rb') as f:\n",
    "    X1=pickle.load(f)\n",
    "with open(PSSM,'rb') as f:\n",
    "    X2=pickle.load(f)\n",
    "with open(DSSP,'rb') as f:\n",
    "    y=pd.DataFrame(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6804ce-81d0-4c3b-9f43-e992253a8640",
   "metadata": {},
   "source": [
    "Add seqID column for grouping by seqID during train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f234fb03-2aae-487a-a796-dc31eda9b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.loc[:,'seqID'] = [c.split('_')[0] for c in X1.index]\n",
    "X2.loc[:,'seqID'] = [c.split('_')[0] for c in X2.index]\n",
    "y.loc[:,'seqID'] = [c.split('_')[0] for c in y.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6722943-7a02-4b48-972d-adf80676c402",
   "metadata": {},
   "source": [
    "Function to load cross val splits from resume.log file, which is an output of scripts/best_shuffle_by_cutoff.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9778c1a-3dc7-4a4a-8437-ada88bdf48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain 7 sets of indices from resume.log, which is an output of shuffle.pl\n",
    "def get_splits(resume_log_file):\n",
    "    val_splits = []\n",
    "    set_idx = -1\n",
    "    cur_set = set() \n",
    "    with open(resume_log_file,'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        for line in lines:\n",
    "            if line.startswith('#SET'):\n",
    "                if set_idx > -1:\n",
    "                    val_splits.append(cur_set)\n",
    "                    cur_set = set()\n",
    "                set_idx += 1\n",
    "            else:\n",
    "                seqID = line.split('/')[-1].replace('.pssm','')\n",
    "                cur_set.add(seqID)\n",
    "        # append last set which is not followed by another line '#SET...'\n",
    "        val_splits.append(cur_set)\n",
    "    assert sum([len(s) for s in val_splits])==1348\n",
    "    return val_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c173c557-4b3c-4117-82ec-5e7546c9597b",
   "metadata": {},
   "source": [
    "Define cross validation routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "49191a42-aa81-41d5-a7f7-881287888ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CV(X,y,**params):\n",
    "    \n",
    "    global DEBUG\n",
    "    \n",
    "    clfs = []\n",
    "    \n",
    "    kf = get_splits('/cluster/gjb_lab/2472402/data/retr231_shuffles/shuffle02/best_shuffle_th_1.log')\n",
    "    \n",
    "    for k in range(7):\n",
    "        \n",
    "        # convert sets into lists\n",
    "        valid_idx = list(kf[k])\n",
    "        train_idx = (set().union(*(split_sets[0:k] + split_sets[k+1:])))\n",
    "\n",
    "        # split data\n",
    "        X_train = X1[X1['seqID'].isin(train_idx)].drop(['seqID'],axis=1)\n",
    "        X_valid = X1[X1['seqID'].isin(valid_idx)].drop(['seqID'],axis=1)\n",
    "        y_train = y[y['seqID'].isin(train_idx)].drop(['seqID'],axis=1)\n",
    "        y_valid = y[y['seqID'].isin(valid_idx)].drop(['seqID'],axis=1)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_valid = scaler.transform(X_valid)\n",
    "        \n",
    "        # convert y into array\n",
    "        y_train = y_train.loc[:,0].ravel()\n",
    "        y_valid = y_valid.loc[:,0].ravel()\n",
    "        \n",
    "        \n",
    "        if DEBUG:\n",
    "            X_train = X_train[::100]\n",
    "            X_valid = X_valid[::10]\n",
    "            y_train = y_train[::100]\n",
    "            y_valid = y_valid[::10]\n",
    "\n",
    "        # generate classifier        \n",
    "        clf = neural_network.MLPClassifier(**params)\n",
    "                \n",
    "        # train classifier\n",
    "        clf.fit(X_train,y_train)\n",
    "\n",
    "        # evaluate model\n",
    "        print(\"Accuracy: \", clf.score(X_valid,y_valid))\n",
    "        \n",
    "        clfs.append(clf)\n",
    "\n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef57f3a7-cfdc-4bf5-ae91-03aeb7f87662",
   "metadata": {},
   "source": [
    "Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3677f9a3-1f76-40ae-826c-196cd0d7e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d8a4f05f-2d14-40ca-a7e1-c9ff2dd1ee71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7094918504314478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6987301587301588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7149805447470817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6952789699570815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6896887159533074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7194132334581773\n",
      "Accuracy:  0.6923601637107776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'alpha' : 1e-5,\n",
    "    'solver' : 'sgd',\n",
    "    'learning_rate' : 'constant',\n",
    "    'momentum' : 0.9,\n",
    "    'nesterovs_momentum' : True,\n",
    "    'max_iter' : 1000,\n",
    "    'shuffle' : True,\n",
    "    'activation' : 'logistic',\n",
    "}\n",
    "clfs = run_CV(X1,y,**params)\n",
    "clfs_list.append(clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ddecbe22-e09d-4e33-b9ea-24cf42f8ce0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.696708213486737\n",
      "Accuracy:  0.6961904761904761\n",
      "Accuracy:  0.711413748378729\n",
      "Accuracy:  0.6928264868179032\n",
      "Accuracy:  0.6913099870298314\n",
      "Accuracy:  0.7147315855181023\n",
      "Accuracy:  0.6988403819918144\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'alpha' : 1e-5,\n",
    "    'solver' : 'adam',\n",
    "    'learning_rate' : 'constant',\n",
    "    'momentum' : 0.9,\n",
    "    'nesterovs_momentum' : True,\n",
    "    'max_iter' : 1000,\n",
    "    'shuffle' : True,\n",
    "    'activation' : 'logistic',\n",
    "}\n",
    "clfs = run_CV(X1,y,**params)\n",
    "clfs_list.append(clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0be55610-7543-459b-ba2d-daea88eed636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7002237136465325\n",
      "Accuracy:  0.7012698412698413\n",
      "Accuracy:  0.711413748378729\n",
      "Accuracy:  0.6958920907418762\n",
      "Accuracy:  0.6932555123216602\n",
      "Accuracy:  0.712234706616729\n",
      "Accuracy:  0.694406548431105\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'alpha' : 1e-5,\n",
    "    'solver' : 'adam',\n",
    "    'learning_rate' : 'invscaling',\n",
    "    'momentum' : 0.9,\n",
    "    'nesterovs_momentum' : True,\n",
    "    'max_iter' : 1000,\n",
    "    'shuffle' : True,\n",
    "    'activation' : 'logistic',\n",
    "    'hidden_layer_sizes' : (100,)\n",
    "}\n",
    "clfs = run_CV(X1,y,**params)\n",
    "clfs_list.append(clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6e5adbb1-3376-4a0e-b71d-dac26b966a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6986257590284436\n",
      "Accuracy:  0.699047619047619\n",
      "Accuracy:  0.7153047989623865\n",
      "Accuracy:  0.6888412017167382\n",
      "Accuracy:  0.6906614785992218\n",
      "Accuracy:  0.7131710362047441\n",
      "Accuracy:  0.7066848567530696\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'alpha' : 1e-3,\n",
    "    'solver' : 'adam',\n",
    "    'learning_rate' : 'invscaling',\n",
    "    'momentum' : 0.9,\n",
    "    'nesterovs_momentum' : True,\n",
    "    'max_iter' : 1000,\n",
    "    'shuffle' : True,\n",
    "    'activation' : 'logistic',\n",
    "    'hidden_layer_sizes' : (100,)\n",
    "}\n",
    "clfs = run_CV(X1,y,**params)\n",
    "clfs_list.append(clfs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ml-env)",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

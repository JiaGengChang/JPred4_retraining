{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f6c904-8556-4457-9dc7-be317408d574",
   "metadata": {},
   "source": [
    "# Combined HMM and PSSM cross validation script "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c432a1a9-a8a7-424a-af01-ceddea783d89",
   "metadata": {},
   "source": [
    "## Setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf4446b-550e-4094-b287-ccbd837e023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "from joblib import Parallel,delayed,Memory\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # suppress INFO and WARNING from tensorflow \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential,layers\n",
    "\n",
    "cachedir = './cachedir'\n",
    "memory = Memory(cachedir,verbose=0)\n",
    "data_dir = '/cluster/gjb_lab/2472402/data/retr231_raw_files/training/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ca5e4-97e9-49fe-bd97-1be744568369",
   "metadata": {},
   "source": [
    "## Current cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab57d8f2-1bf6-4d4e-bb3a-e67549b9f1c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_splits = get_splits('/cluster/gjb_lab/2472402/data/retr231_shuffles/shuffle02/best_shuffle_th_1.log')\n",
    "def split_train_valid(val_fold):\n",
    "    valid_set = val_splits[val_fold]\n",
    "    train_set = set().union(*(val_splits[:val_fold] + val_splits[val_fold+1:]))\n",
    "    return train_set, valid_set\n",
    "\n",
    "fold=3\n",
    "train_set, valid_set = split_train_valid(fold)\n",
    "#train_seq_networks(train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac3b5b-7311-4b8e-967d-467e78f86847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_struct_networks(train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "750ad9a1-da63-435c-b17f-465cb0ab4bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./fold3/model_pssm1_NN_63/assets\n",
      "INFO:tensorflow:Assets written to: ./fold3/model_hmm1_NN_86/assets\n",
      "CPU times: user 26min 44s, sys: 3min 5s, total: 29min 49s\n",
      "Wall time: 6min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_seq_networks(train_set, valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf9004f-bcb4-4db7-b607-fa9d06a1d11b",
   "metadata": {},
   "source": [
    "## Current function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adc67d2a-ed1b-4d0e-bbe4-6953e7bc4200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training loop \n",
    "# Called from within cross-validation loop\n",
    "# once per cross-validation\n",
    "def train_seq_networks(train_idx_set,valid_idx_set):\n",
    "    \n",
    "    train_data = generate_data(train_idx_set)\n",
    "    valid_data = generate_data(valid_idx_set)\n",
    "    \n",
    "    # can have separate loss fn and optimizer for hmm and pssm in future\n",
    "    loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=5e-3)\n",
    "    ru = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05) # follow jpred\n",
    "    hmm_metric = keras.metrics.CategoricalAccuracy()\n",
    "    pssm_metric = keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    hmm1_NN = Sequential([\n",
    "        layers.Dense(units = 100, input_shape=[408], activation='sigmoid',kernel_initializer=ru),\n",
    "        layers.Dense(units = 3, activation ='softmax',kernel_initializer=ru),\n",
    "    ])\n",
    "    pssm1_NN = Sequential([\n",
    "        layers.Dense(units = 100, input_shape=[340], activation='sigmoid',kernel_initializer=ru),\n",
    "        layers.Dense(units = 3, activation ='softmax',kernel_initializer=ru),\n",
    "    ])\n",
    "    \n",
    "    hmm1_NN.compile(loss=loss_fn,optimizer=optimizer, metrics=[hmm_metric])\n",
    "    pssm1_NN.compile(loss=loss_fn,optimizer=optimizer, metrics=[pssm_metric])\n",
    "     \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=(None,3)),\n",
    "                         tf.TensorSpec(shape=(None,408)),\n",
    "                         tf.TensorSpec(shape=(None,340)),])\n",
    "    def train_step(labels,hmm_data,pssm_data):\n",
    "        # forward pass on hmm1 neural network\n",
    "        with tf.GradientTape() as hmm_tape:\n",
    "            hmm_proba = hmm1_NN(hmm_data, training=True)\n",
    "            hmm_loss = loss_fn(labels, hmm_proba)\n",
    "\n",
    "        # forward pass on pssm1 neural network\n",
    "        with tf.GradientTape() as pssm_tape:\n",
    "            pssm_proba = pssm1_NN(pssm_data, training=True)\n",
    "            pssm_loss = loss_fn(labels, pssm_proba)\n",
    "\n",
    "        hmm_grads = hmm_tape.gradient(hmm_loss, hmm1_NN.trainable_weights)\n",
    "        pssm_grads = pssm_tape.gradient(pssm_loss, pssm1_NN.trainable_weights)\n",
    "\n",
    "        optimizer.apply_gradients(zip(hmm_grads, hmm1_NN.trainable_weights))\n",
    "        optimizer.apply_gradients(zip(pssm_grads, pssm1_NN.trainable_weights))\n",
    "        \n",
    "        return hmm_loss,pssm_loss\n",
    "\n",
    "     \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=(None,3)),\n",
    "                                  tf.TensorSpec(shape=(None,408)),\n",
    "                                  tf.TensorSpec(shape=(None,340)),])\n",
    "    def valid_step(labels,hmm_data, pssm_data):\n",
    "        hmm_proba = hmm1_NN(hmm_data, training=False)\n",
    "        pssm_proba = pssm1_NN(pssm_data, training=False)\n",
    "        \n",
    "        hmm_metric.update_state(labels, hmm_proba)\n",
    "        pssm_metric.update_state(labels, pssm_proba)\n",
    "        \n",
    "        # return validation loss for early stopping\n",
    "        hmm_loss = loss_fn(labels, hmm_proba)\n",
    "        pssm_loss = loss_fn(labels, pssm_proba)\n",
    "        return hmm_loss, pssm_loss\n",
    "    \n",
    "    # print to log.txt\n",
    "    def print(*args,**kwargs):\n",
    "        with open('./fold%d/log.txt' % fold,'a') as f:\n",
    "            kwargs['file']=f\n",
    "            return __builtin__.print(*args,**kwargs)\n",
    "    \n",
    "    print('Training sequence to structure networks...')\n",
    "    hmm_valid_loss = [] # for early stopping\n",
    "    pssm_valid_loss = []\n",
    "    hmm_training_finished = False\n",
    "    pssm_training_finished = False\n",
    "    epochs = 1000\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        epoch_hmm_Tloss = 0\n",
    "        epoch_pssm_Tloss = 0\n",
    "        train_seqIDs = list(train_data.keys())\n",
    "        random.shuffle(train_seqIDs)\n",
    "        for step, seqID in enumerate(train_seqIDs):\n",
    "            \n",
    "            batch_size = len(train_data[seqID])\n",
    "            labels = train_data[seqID][:,0:3]\n",
    "            hmm_data = train_data[seqID][:,3:411]\n",
    "            pssm_data = train_data[seqID][:,411:751]\n",
    "            labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "            hmm_data = tf.convert_to_tensor(hmm_data, dtype=tf.float32)\n",
    "            pssm_data = tf.convert_to_tensor(pssm_data, dtype=tf.float32)\n",
    "            \n",
    "            hmm_loss, pssm_loss = train_step(labels,hmm_data,pssm_data)\n",
    "            \n",
    "            epoch_hmm_Tloss += hmm_loss/batch_size\n",
    "            epoch_pssm_Tloss += pssm_loss/batch_size\n",
    "        \n",
    "        # calculate training accuracy\n",
    "        for step, seqID in enumerate(train_seqIDs):\n",
    "            batch_size = len(train_data[seqID])\n",
    "            labels = train_data[seqID][:,0:3]\n",
    "            hmm_data = train_data[seqID][:,3:411]\n",
    "            pssm_data = train_data[seqID][:,411:751]\n",
    "            labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "            hmm_data = tf.convert_to_tensor(hmm_data, dtype=tf.float32)\n",
    "            pssm_data = tf.convert_to_tensor(pssm_data, dtype=tf.float32)\n",
    "            \n",
    "            valid_step(labels,hmm_data,pssm_data)\n",
    "        \n",
    "        hmm_acc_train = hmm_metric.result()\n",
    "        pssm_acc_train = pssm_metric.result()\n",
    "        hmm_metric.reset_states()\n",
    "        pssm_metric.reset_states()\n",
    "        \n",
    "        # end of epoch validation\n",
    "        epoch_hmm_loss = 0\n",
    "        epoch_pssm_loss = 0\n",
    "        valid_seqIDs = list(valid_data.keys())\n",
    "        random.shuffle(valid_seqIDs)\n",
    "        for step, seqID in enumerate(valid_seqIDs):\n",
    "            batch_size = len(valid_data[seqID])\n",
    "            labels = valid_data[seqID][:,:3]\n",
    "            hmm_data = valid_data[seqID][:,3:411]\n",
    "            pssm_data = valid_data[seqID][:,411:751]\n",
    "            labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "            hmm_data = tf.convert_to_tensor(hmm_data, dtype=tf.float32)\n",
    "            pssm_data = tf.convert_to_tensor(pssm_data, dtype=tf.float32)\n",
    "            \n",
    "            hmm_loss, pssm_loss = valid_step(labels,hmm_data,pssm_data)\n",
    "            \n",
    "            epoch_hmm_loss += hmm_loss/batch_size\n",
    "            epoch_pssm_loss += pssm_loss/batch_size\n",
    "        \n",
    "        hmm_valid_loss.append(epoch_hmm_loss)\n",
    "        pssm_valid_loss.append(epoch_pssm_loss)\n",
    "        hmm_acc_valid = hmm_metric.result()\n",
    "        pssm_acc_valid = pssm_metric.result()\n",
    "        hmm_metric.reset_states()\n",
    "        pssm_metric.reset_states()\n",
    "        \n",
    "        print(\n",
    "            \"Epoch %d HMM_acc %.4f PSSM_acc %.4f HMM_loss %.4f PSSM_loss %.4f HMM_Tacc %.4f PSSM_Tacc %.4f HMM_Tloss %.4f PSSM_Tloss %.4f\" \n",
    "            % (\n",
    "            epoch, hmm_acc_valid, pssm_acc_valid, epoch_hmm_loss, epoch_pssm_loss,\n",
    "                hmm_acc_train, pssm_acc_train, epoch_hmm_Tloss, epoch_pssm_Tloss\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # check for early stopping\n",
    "        hmm_stopEarly = Callback_EarlyStopping(hmm_valid_loss, min_delta=1e-5, patience=20, mode='min')\n",
    "        pssm_stopEarly = Callback_EarlyStopping(pssm_valid_loss, min_delta=1e-5, patience=20, mode='min')\n",
    "        \n",
    "        if not hmm_training_finished:\n",
    "            if hmm_stopEarly:\n",
    "                print(\"Early stopping for hmm1_NN at epoch %d/%d\" % (epoch,epochs))\n",
    "                hmm1_NN.save('./fold%d/model_hmm1_NN_%d' % (fold,epoch))\n",
    "                hmm_training_finished = True\n",
    "        \n",
    "        if not pssm_training_finished:\n",
    "            if pssm_stopEarly:\n",
    "                print(\"Early stopping for pssm1_NN at epoch %d/%d\" % (epoch,epochs))\n",
    "                pssm1_NN.save('./fold%d/model_pssm1_NN_%d' % (fold,epoch))\n",
    "                pssm_training_finished = True\n",
    "        \n",
    "        if hmm_training_finished and pssm_training_finished:\n",
    "            print(\"Training finished at epoch %d/%d\" % (epoch,epochs))\n",
    "            break\n",
    "        \n",
    "    if not hmm_training_finished and not pssm_training_finished:\n",
    "        print(\"Training finished at epoch %d with no early stopping\" % epochs)\n",
    "        hmm1_NN.save('./fold%d/model_hmm1_NN_%d' % (fold,epochs))\n",
    "        pssm1_NN.save('./fold%d/model_pssm1_NN_%d' % (fold,epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea10232-0e93-4c08-90c9-8f28f0b9302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/59438904/applying-callbacks-in-a-custom-training-loop-in-tensorflow-2-0\n",
    "# Matthew Thomas\n",
    "# keras implementation of early stopping\n",
    "def Callback_EarlyStopping(MetricList, min_delta=0.1, patience=20, mode='min'):\n",
    "    #No early stopping for the first patience epochs \n",
    "    if len(MetricList) <= patience:\n",
    "        return False\n",
    "    \n",
    "    min_delta = abs(min_delta)\n",
    "    if mode == 'min':\n",
    "        min_delta *= -1\n",
    "    else:\n",
    "        min_delta *= 1\n",
    "    \n",
    "    #last patience epochs \n",
    "    last_patience_epochs = [x + min_delta for x in MetricList[::-1][1:patience + 1]]\n",
    "    current_metric = MetricList[::-1][0]\n",
    "    \n",
    "    if mode == 'min':\n",
    "        return current_metric >= max(last_patience_epochs)\n",
    "    else:\n",
    "        return current_metric <= min(last_patience_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06923f0d-3764-4e38-a7ca-e15dc300a986",
   "metadata": {},
   "source": [
    "## Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2385bf88-69bb-40b1-bd6d-d5dd676dc591",
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def generate_data_for_NN2(set_of_seqID):\n",
    "    hmm1_NN = keras.models.load_model('./hmm1_NN')\n",
    "    pssm1_NN = keras.models.load_model('./pssm1_NN')\n",
    "    data_in = generate_data(set_of_seqID)\n",
    "    \n",
    "    def process_seqID(data):\n",
    "        label = data[:,:3]\n",
    "        hmm1_in = data[:,3:411]\n",
    "        pssm1_in = data[:,411:751]\n",
    "        hmm1_out = hmm1_NN(hmm1_in, training=False).numpy()\n",
    "        pssm1_out = pssm1_NN(pssm1_in, training=False).numpy()\n",
    "        hmm2_in = sliding_window(hmm1_out, flank=9)\n",
    "        pssm2_in = sliding_window(pssm1_out, flank=9)\n",
    "        assert hmm2_in.shape[1]==57\n",
    "        assert pssm2_in.shape[1]==57\n",
    "        result = np.concatenate([label,hmm2_in,pssm2_in],axis=1)\n",
    "        return result\n",
    "    \n",
    "    arr_dict = {}\n",
    "    start_time = time.time()\n",
    "    for seqID in set_of_seqID:\n",
    "        arr_dict[seqID] = process_seqID(data_in[seqID])\n",
    "    print(\"Took %s seconds to process %d seqIDs\" % (time.time() - start_time, len(set_of_seqID)))\n",
    "    return arr_dict\n",
    "    \n",
    "def train_struct_networks(train_idx_set,valid_idx_set):\n",
    "    \n",
    "    train_data = generate_data_for_NN2(train_idx_set)\n",
    "    valid_data = generate_data_for_NN2(valid_idx_set)\n",
    "    \n",
    "    # can have separate loss fn and optimizer for hmm and pssm in future\n",
    "    loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "    hmm_metric_train = keras.metrics.CategoricalAccuracy()\n",
    "    hmm_metric_valid = keras.metrics.CategoricalAccuracy()\n",
    "    pssm_metric_train = keras.metrics.CategoricalAccuracy()\n",
    "    pssm_metric_valid = keras.metrics.CategoricalAccuracy()\n",
    "    ru = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05) # follow jpred\n",
    "\n",
    "    hmm2_NN = Sequential([\n",
    "        layers.Dense(units = 100, input_shape=[57], activation='sigmoid',kernel_initializer=ru),\n",
    "        layers.Dense(units = 3, activation ='softmax',kernel_initializer=ru),\n",
    "    ])\n",
    "    pssm2_NN = Sequential([\n",
    "        layers.Dense(units = 100, input_shape=[57], activation='sigmoid',kernel_initializer=ru),\n",
    "        layers.Dense(units = 3, activation ='softmax',kernel_initializer=ru),\n",
    "        ])\n",
    "    \n",
    "    hmm2_NN.compile(loss=loss_fn,optimizer=optimizer,metrics=[hmm_metric_train,hmm_metric_valid])\n",
    "    pssm2_NN.compile(loss=loss_fn,optimizer=optimizer,metrics=[pssm_metric_train,pssm_metric_valid])\n",
    "    \n",
    "    @tf.function( # prevent retracing\n",
    "        input_signature=[tf.TensorSpec(shape=(None,3)),\n",
    "                         tf.TensorSpec(shape=(None,57)),\n",
    "                         tf.TensorSpec(shape=(None,57)),\n",
    "                        ]\n",
    "    )\n",
    "    def train_step(labels,hmm_data, pssm_data):\n",
    "        # forward pass on hmm2 neural network\n",
    "        with tf.GradientTape() as hmm_tape:\n",
    "            hmm_proba = hmm2_NN(hmm_data, training=True)\n",
    "            hmm_loss = loss_fn(labels, hmm_proba)\n",
    "\n",
    "        # forward pass on pssm2 neural network\n",
    "        with tf.GradientTape() as pssm_tape:\n",
    "            pssm_proba = pssm2_NN(pssm_data, training=True)\n",
    "            pssm_loss = loss_fn(labels, pssm_proba)\n",
    "\n",
    "        hmm_grads = hmm_tape.gradient(hmm_loss, hmm2_NN.trainable_weights)\n",
    "        pssm_grads = pssm_tape.gradient(pssm_loss, pssm2_NN.trainable_weights)\n",
    "\n",
    "        optimizer.apply_gradients(zip(hmm_grads, hmm2_NN.trainable_weights))\n",
    "        optimizer.apply_gradients(zip(pssm_grads, pssm2_NN.trainable_weights))\n",
    "\n",
    "        hmm_metric_train.update_state(labels, hmm_proba)\n",
    "        pssm_metric_train.update_state(labels, pssm_proba)\n",
    "\n",
    "        return hmm_loss, pssm_loss\n",
    "\n",
    "    @tf.function( \n",
    "        input_signature=[tf.TensorSpec(shape=(None,3)),\n",
    "                         tf.TensorSpec(shape=(None,57)),\n",
    "                         tf.TensorSpec(shape=(None,57)),\n",
    "                        ]\n",
    "    )\n",
    "    def valid_step(labels,hmm_data, pssm_data):\n",
    "        hmm_proba = hmm2_NN(hmm_data, training=False)\n",
    "        pssm_proba = pssm2_NN(pssm_data, training=False)\n",
    "        hmm_metric_valid.update_state(labels, hmm_proba)\n",
    "        pssm_metric_valid.update_state(labels, pssm_proba)\n",
    "\n",
    "    print('Training structure to structure networks...')\n",
    "    \n",
    "    epochs = 200\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_seqIDs = list(train_data.keys())\n",
    "        random.shuffle(train_seqIDs)\n",
    "        for step, seqID in enumerate(train_seqIDs):\n",
    "            batch_size = len(train_data[seqID])\n",
    "            labels = train_data[seqID][:,0:3]\n",
    "            hmm_data = train_data[seqID][:,3:60]\n",
    "            pssm_data = train_data[seqID][:,60:117]\n",
    "            \n",
    "            labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "            hmm_data = tf.convert_to_tensor(hmm_data, dtype=tf.float32)\n",
    "            pssm_data = tf.convert_to_tensor(pssm_data, dtype=tf.float32)\n",
    "            hmm_loss,pssm_loss = train_step(labels,hmm_data,pssm_data)\n",
    "            \n",
    "            if step % 200 == 0:\n",
    "                print('HMM2 loss (for one domain) at step %d: %.4f' % (step, hmm_loss/batch_size))\n",
    "                print('PSI2 loss (for one domain) at step %d: %.4f' % (step, pssm_loss/batch_size))\n",
    "                \n",
    "        hmm_acc = hmm_metric_train.result()\n",
    "        pssm_acc = pssm_metric_train.result()\n",
    "        print(\"HMM2 training accuracy over epoch: %.4f\" % (hmm_acc,))\n",
    "        print(\"PSI2 training accuracy over epoch: %.4f\" % (pssm_acc,))\n",
    "        hmm_metric_train.reset_states()\n",
    "        pssm_metric_train.reset_states()\n",
    "        \n",
    "        # end of epoch validation\n",
    "        print(\"\\nEnd of epoch %d validation\" % (epoch,))\n",
    "        valid_seqIDs = list(valid_data.keys())\n",
    "        random.shuffle(valid_seqIDs)\n",
    "        for step, seqID in enumerate(valid_seqIDs):\n",
    "            labels = valid_data[seqID][:,:3]\n",
    "            hmm_data = valid_data[seqID][:,3:60]\n",
    "            pssm_data = valid_data[seqID][:,60:117]\n",
    "            labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "            hmm_data = tf.convert_to_tensor(hmm_data, dtype=tf.float32)\n",
    "            pssm_data = tf.convert_to_tensor(pssm_data, dtype=tf.float32)\n",
    "            valid_step(labels,hmm_data,pssm_data)\n",
    "            \n",
    "        hmm_acc = hmm_metric_valid.result()\n",
    "        pssm_acc = pssm_metric_valid.result()\n",
    "        print(\"HMM2 validation accuracy over epoch: %.4f\" % (hmm_acc,))\n",
    "        print(\"PSI2 validation accuracy over epoch: %.4f\" % (pssm_acc,))\n",
    "        hmm_metric_valid.reset_states()\n",
    "        pssm_metric_valid.reset_states()\n",
    "        \n",
    "        #end of epoch\n",
    "        print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n",
    "    \n",
    "# run this function to process multiple IDs\n",
    "def generate_data(seqIDs):\n",
    "    arr_list = Parallel(n_jobs=-1,verbose=0)(delayed(process_seqID)(seqID) for seqID in seqIDs)\n",
    "    return {seqID: arr for seqID, arr in zip(seqIDs,arr_list)}\n",
    "\n",
    "# get splits from resume.log generated by Perl shuffling scripts \n",
    "# returns sets of strings of seqIDs \n",
    "@memory.cache\n",
    "def get_splits(resume_log_file):\n",
    "    val_splits = []\n",
    "    set_idx = -1\n",
    "    cur_set = set() \n",
    "    with open(resume_log_file,'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        for line in lines:\n",
    "            if line.startswith('#SET'):\n",
    "                if set_idx > -1:\n",
    "                    val_splits.append(cur_set)\n",
    "                    cur_set = set()\n",
    "                set_idx += 1\n",
    "            else:\n",
    "                seqID = line.split('/')[-1].replace('.pssm','')\n",
    "                cur_set.add(seqID)\n",
    "        # append last set which is not followed by another line '#SET...'\n",
    "        val_splits.append(cur_set)\n",
    "    assert sum([len(s) for s in val_splits])==1348\n",
    "    return val_splits\n",
    "\n",
    "# produces a single numpy array for each sequence\n",
    "@memory.cache\n",
    "def process_seqID(seqID):\n",
    "    data_dir = '/cluster/gjb_lab/2472402/data/retr231_raw_files/training/'\n",
    "    hmm_path = data_dir + seqID + '.hmm'\n",
    "    pssm_path = data_dir + seqID + '.pssm'\n",
    "    dssp_path = data_dir + seqID + '.dssp'\n",
    "    assert os.path.exists(pssm_path)\n",
    "    hmm = np.loadtxt(hmm_path,delimiter=' ')\n",
    "    hmm = sliding_window(hmm,flank=8)\n",
    "    pssm = np.loadtxt(pssm_path,delimiter=' ')\n",
    "    pssm = sliding_window(pssm,flank=8)\n",
    "    dssp = get_dssp(dssp_path)\n",
    "    res = np.concatenate([dssp,hmm,pssm],axis=1)\n",
    "    return res\n",
    "\n",
    "# in: np array. out: np array linearized over sliding window\n",
    "def sliding_window(array, flank):\n",
    "    assert flank > 0\n",
    "    assert type(array) is np.ndarray\n",
    "    assert np.logical_not(np.isnan(np.sum(array)))\n",
    "    nrow = array.shape[0]\n",
    "    assert nrow > 0\n",
    "    ncol = array.shape[1]\n",
    "    assert ncol > 0\n",
    "    res = np.empty(shape=(nrow, (2*flank+1)*ncol),dtype=np.float32)\n",
    "    res[:] = np.nan\n",
    "    for i in list(range(0,nrow)):\n",
    "        s, e = i-flank, i+flank+1\n",
    "        k = 0;\n",
    "        for j in list(range(s,e)):\n",
    "            if (j < 0 or j >= nrow):\n",
    "                res[i, k:k+ncol] = 0\n",
    "            else:\n",
    "                assert np.logical_not(np.isnan(np.sum(array[j])))\n",
    "                assert array[j].shape == (ncol,)\n",
    "                res[i, k:k+ncol] = array[j]\n",
    "            k += ncol\n",
    "    assert np.logical_not(np.isnan(np.sum(res)))\n",
    "    assert res.shape == (nrow, (2*flank+1)*ncol)\n",
    "    return res\n",
    "\n",
    "def encode(s):\n",
    "    res = np.empty(shape=(len(s),3),dtype=np.byte)\n",
    "    res[:] = np.nan\n",
    "    for i in range(0,len(s)):\n",
    "        if s[i] == 'H':\n",
    "            res[i] = np.array([0,1,0])\n",
    "        else:\n",
    "            if s[i] == 'E':\n",
    "                res[i] = np.array([1,0,0])\n",
    "            else:\n",
    "                assert s[i]\n",
    "                res[i] = np.array([0,0,1])\n",
    "    assert not np.isnan(np.sum(res))\n",
    "    return res\n",
    "\n",
    "def get_dssp(dssp_path):\n",
    "    with open(dssp_path,'r') as f:\n",
    "        dssp = f.read().strip()\n",
    "    return encode(dssp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c4f089-6c1b-4a9c-9a28-e598f8705f6f",
   "metadata": {},
   "source": [
    "## Check sanity of data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af8740d0-941d-4d43-8a40-7e82b0bc20c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_DEBUG=True\n",
    "if IS_DEBUG:\n",
    "    # not actually needed but it does a sanity check on input data\n",
    "    dssp_files = glob.glob(data_dir + '*.dssp')\n",
    "    hmm_files = glob.glob(data_dir + '*.hmm')\n",
    "    pssm_files = glob.glob(data_dir + '*.pssm')\n",
    "    seq_files = glob.glob(data_dir + '*.fasta')\n",
    "    seqIDs = [f.split('/')[-1][:-5] for f in dssp_files]\n",
    "    set_seqIDs_all = set([f.split('/')[-1][:-6] for f in seq_files])\n",
    "    set_seqIDs = set(seqIDs)\n",
    "    unused_seqIDs = set_seqIDs ^ set_seqIDs_all # 9 of them are unused\n",
    "    seq_files = [f for f in seq_files if f.split('/')[-1][:-6] not in unused_seqIDs]\n",
    "    assert all([len(x)==1348 for x in [dssp_files,hmm_files,pssm_files,seq_files,seqIDs]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385470fd-f33d-49f8-ab96-3f0bab1b2ec2",
   "metadata": {},
   "source": [
    "## Work in progress functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b6ef5d-d36e-4223-977b-c01d347ead87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main cross validation function here\n",
    "def run_cross_validation(val_splits):\n",
    "    \n",
    "    def split_train_valid(val_fold):\n",
    "        valid_set = val_splits[val_fold]\n",
    "        train_set = set().union(*(val_splits[:val_fold] + val_splits[val_fold+1:]))\n",
    "        return train_set, valid_set    \n",
    "    \n",
    "    for fold in [3]:\n",
    "        train_set, valid_set = split_train_valid(fold)\n",
    "        train_seq_networks(train_set, valid_set)\n",
    "        train_struct_networks(train_set, valid_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ml-env)",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

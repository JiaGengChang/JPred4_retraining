{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f6c904-8556-4457-9dc7-be317408d574",
   "metadata": {},
   "source": [
    "# Combined HMM and PSSM cross validation script "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c432a1a9-a8a7-424a-af01-ceddea783d89",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf4446b-550e-4094-b287-ccbd837e023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import builtins\n",
    "from joblib import Parallel,delayed,Memory\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # suppress INFO and WARNING from tensorflow \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential,layers\n",
    "\n",
    "cachedir = './cachedir'\n",
    "memory = Memory(cachedir,verbose=0)\n",
    "data_dir = '/cluster/gjb_lab/2472402/data/retr231_raw_files/training/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf9004f-bcb4-4db7-b607-fa9d06a1d11b",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d4c1e-596e-4c03-b9d1-bb10a4b644a4",
   "metadata": {},
   "source": [
    "### Train structure layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f6482b-1e1e-4729-9a6f-6a4e92e122d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_struct_networks(train_idx_set,valid_idx_set,**params):\n",
    "    \n",
    "    lr = params['learning_rate']\n",
    "    md = params['min_delta']\n",
    "    fold_dir = params['fold_dir']\n",
    "    epochs = params['epochs'] if params['epochs'] else 300\n",
    "    \n",
    "    train_data = generate_data_for_NN2(train_idx_set,fold_dir)\n",
    "    valid_data = generate_data_for_NN2(valid_idx_set,fold_dir)\n",
    "    \n",
    "    # can have separate loss fn and optimizer for hmm and pssm in future\n",
    "    loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
    "    hmm_metric_train = keras.metrics.CategoricalAccuracy()\n",
    "    hmm_metric_valid = keras.metrics.CategoricalAccuracy()\n",
    "    pssm_metric_train = keras.metrics.CategoricalAccuracy()\n",
    "    pssm_metric_valid = keras.metrics.CategoricalAccuracy()\n",
    "    ru = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05) # follow jpred\n",
    "\n",
    "    hmm2_NN = Sequential([\n",
    "        layers.Dense(units = 100, input_shape=[57], activation='sigmoid',kernel_initializer=ru),\n",
    "        layers.Dense(units = 3, activation ='softmax',kernel_initializer=ru),\n",
    "    ])\n",
    "    pssm2_NN = Sequential([\n",
    "        layers.Dense(units = 100, input_shape=[57], activation='sigmoid',kernel_initializer=ru),\n",
    "        layers.Dense(units = 3, activation ='softmax',kernel_initializer=ru),\n",
    "        ])\n",
    "    \n",
    "    @tf.function( # prevent retracing\n",
    "        input_signature=[tf.TensorSpec(shape=(None,3)),\n",
    "                         tf.TensorSpec(shape=(None,57)),\n",
    "                         tf.TensorSpec(shape=(None,57)),\n",
    "                        ]\n",
    "    )\n",
    "    def train_step(labels,hmm_data, pssm_data):\n",
    "        # forward pass on hmm2 neural network\n",
    "        with tf.GradientTape() as hmm_tape:\n",
    "            hmm_proba = hmm2_NN(hmm_data, training=True)\n",
    "            hmm_loss = loss_fn(labels, hmm_proba)\n",
    "\n",
    "        # forward pass on pssm2 neural network\n",
    "        with tf.GradientTape() as pssm_tape:\n",
    "            pssm_proba = pssm2_NN(pssm_data, training=True)\n",
    "            pssm_loss = loss_fn(labels, pssm_proba)\n",
    "\n",
    "        hmm_grads = hmm_tape.gradient(hmm_loss, hmm2_NN.trainable_weights)\n",
    "        pssm_grads = pssm_tape.gradient(pssm_loss, pssm2_NN.trainable_weights)\n",
    "\n",
    "        optimizer.apply_gradients(zip(hmm_grads, hmm2_NN.trainable_weights))\n",
    "        optimizer.apply_gradients(zip(pssm_grads, pssm2_NN.trainable_weights))\n",
    "\n",
    "        hmm_metric_train.update_state(labels, hmm_proba)\n",
    "        pssm_metric_train.update_state(labels, pssm_proba)\n",
    "\n",
    "        return hmm_loss, pssm_loss\n",
    "\n",
    "    @tf.function( \n",
    "        input_signature=[tf.TensorSpec(shape=(None,3)),\n",
    "                         tf.TensorSpec(shape=(None,57)),\n",
    "                         tf.TensorSpec(shape=(None,57)),\n",
    "                        ]\n",
    "    )\n",
    "    def valid_step(labels,hmm_data, pssm_data):\n",
    "        hmm_proba = hmm2_NN(hmm_data, training=False)\n",
    "        pssm_proba = pssm2_NN(pssm_data, training=False)\n",
    "        hmm_metric_valid.update_state(labels, hmm_proba)\n",
    "        pssm_metric_valid.update_state(labels, pssm_proba)\n",
    "        \n",
    "        # return validation loss for early stopping\n",
    "        hmm_loss = loss_fn(labels, hmm_proba)\n",
    "        pssm_loss = loss_fn(labels, pssm_proba)\n",
    "        \n",
    "        return hmm_loss, pssm_loss\n",
    "    \n",
    "    # print to log2.txt\n",
    "    def print(*args,**kwargs):\n",
    "        with open(fold_dir + 'log2.txt','a') as f:\n",
    "            kwargs['file']=f\n",
    "            return builtins.print(*args,**kwargs)\n",
    "        \n",
    "    print('Training structure to structure networks...')\n",
    "    \n",
    "    hmm_valid_loss = [] # for early stopping\n",
    "    pssm_valid_loss = []\n",
    "    hmm_training_finished = False\n",
    "    pssm_training_finished = False\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        epoch_hmm_Tloss = 0\n",
    "        epoch_pssm_Tloss = 0\n",
    "        train_seqIDs = list(train_data.keys())\n",
    "        random.shuffle(train_seqIDs)\n",
    "        for step, seqID in enumerate(train_seqIDs):\n",
    "            batch_size = len(train_data[seqID])\n",
    "            labels = train_data[seqID][:,0:3]\n",
    "            hmm_data = train_data[seqID][:,3:60]\n",
    "            pssm_data = train_data[seqID][:,60:117]\n",
    "            \n",
    "            labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "            hmm_data = tf.convert_to_tensor(hmm_data, dtype=tf.float32)\n",
    "            pssm_data = tf.convert_to_tensor(pssm_data, dtype=tf.float32)\n",
    "            \n",
    "            hmm_loss,pssm_loss = train_step(labels,hmm_data,pssm_data)\n",
    "            \n",
    "            epoch_hmm_Tloss += hmm_loss/batch_size\n",
    "            epoch_pssm_Tloss += pssm_loss/batch_size\n",
    "            \n",
    "        hmm_acc_train = hmm_metric_train.result()\n",
    "        pssm_acc_train = pssm_metric_train.result()\n",
    "        hmm_metric_train.reset_states()\n",
    "        pssm_metric_train.reset_states()\n",
    "        \n",
    "        # end of epoch validation\n",
    "        epoch_hmm_loss = 0\n",
    "        epoch_pssm_loss = 0\n",
    "        valid_seqIDs = list(valid_data.keys())\n",
    "        random.shuffle(valid_seqIDs)\n",
    "        for step, seqID in enumerate(valid_seqIDs):\n",
    "            batch_size = len(valid_data[seqID])\n",
    "            labels = valid_data[seqID][:,:3]\n",
    "            hmm_data = valid_data[seqID][:,3:60]\n",
    "            pssm_data = valid_data[seqID][:,60:117]\n",
    "            labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "            hmm_data = tf.convert_to_tensor(hmm_data, dtype=tf.float32)\n",
    "            pssm_data = tf.convert_to_tensor(pssm_data, dtype=tf.float32)\n",
    "            \n",
    "            hmm_loss,pssm_loss = valid_step(labels,hmm_data,pssm_data)\n",
    "            \n",
    "            epoch_hmm_loss += hmm_loss/batch_size\n",
    "            epoch_pssm_loss += pssm_loss/batch_size\n",
    "        \n",
    "        hmm_acc_valid = hmm_metric_valid.result()\n",
    "        pssm_acc_valid = pssm_metric_valid.result()\n",
    "        hmm_metric_valid.reset_states()\n",
    "        pssm_metric_valid.reset_states()\n",
    "        \n",
    "        print(\n",
    "            \"Epoch %d HMM_acc %.4f PSSM_acc %.4f HMM_loss %.4f PSSM_loss %.4f HMM_Tacc %.4f PSSM_Tacc %.4f HMM_Tloss %.4f PSSM_Tloss %.4f\" \n",
    "            % (\n",
    "            epoch, hmm_acc_valid, pssm_acc_valid, epoch_hmm_loss, epoch_pssm_loss,\n",
    "                hmm_acc_train, pssm_acc_train, epoch_hmm_Tloss, epoch_pssm_Tloss\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # check for early stopping\n",
    "        hmm_stopEarly = Callback_EarlyStopping(hmm_valid_loss, min_delta=md, patience=20)\n",
    "        pssm_stopEarly = Callback_EarlyStopping(pssm_valid_loss, min_delta=md, patience=20)\n",
    "        \n",
    "        if not hmm_training_finished:\n",
    "            if hmm_stopEarly:\n",
    "                print(\"Early stopping for hmm2_NN at epoch %d/%d\" % (epoch,epochs))\n",
    "                hmm1_NN.save(fold_dir + 'hmm2_NN')\n",
    "                hmm_training_finished = True\n",
    "        \n",
    "        if not pssm_training_finished:\n",
    "            if pssm_stopEarly:\n",
    "                print(\"Early stopping for pssm2_NN at epoch %d/%d\" % (epoch,epochs))\n",
    "                pssm1_NN.save(fold_dir + 'pssm2_NN')\n",
    "                pssm_training_finished = True\n",
    "        \n",
    "        if hmm_training_finished and pssm_training_finished:\n",
    "            print(\"Training finished at epoch %d/%d\" % (epoch,epochs))\n",
    "            break\n",
    "            \n",
    "    print(\"Training finished at epoch %d\" % epochs)\n",
    "    if hmm_training_finished:\n",
    "        hmm2_NN.save(fold_dir + 'hmm2_NN_%d' % (epochs))\n",
    "    else:\n",
    "        hmm2_NN.save(fold_dir + 'hmm2_NN')\n",
    "    if pssm_training_finished:\n",
    "        pssm2_NN.save(fold_dir + 'pssm2_NN_%d' % (epochs))\n",
    "    else:\n",
    "        pssm2_NN.save(fold_dir + 'pssm2_NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fe425c-af47-4376-a3a5-476753f252ec",
   "metadata": {},
   "source": [
    "### Train sequence layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcb921f7-a2f7-441c-936f-a28727b95819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training loop \n",
    "# Called from within cross-validation loop\n",
    "# once per cross-validation\n",
    "def train_seq_networks(train_idx_set,valid_idx_set,**params):\n",
    "    \n",
    "    lr = params['learning_rate']\n",
    "    md = params['min_delta']\n",
    "    fold_dir = params['fold_dir']\n",
    "    epochs = params['epochs'] if params['epochs'] else 300\n",
    "    \n",
    "    train_data = generate_data(train_idx_set)\n",
    "    valid_data = generate_data(valid_idx_set)\n",
    "    \n",
    "    # can have separate loss fn and optimizer for hmm and pssm in future\n",
    "    loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
    "    ru = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05) # follow jpred\n",
    "    hmm_metric = keras.metrics.CategoricalAccuracy()\n",
    "    pssm_metric = keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    hmm1_NN = Sequential([\n",
    "        layers.Dense(units = 100, input_shape=[408], activation='sigmoid',kernel_initializer=ru),\n",
    "        layers.Dense(units = 3, activation ='softmax',kernel_initializer=ru),\n",
    "    ])\n",
    "    pssm1_NN = Sequential([\n",
    "        layers.Dense(units = 100, input_shape=[340], activation='sigmoid',kernel_initializer=ru),\n",
    "        layers.Dense(units = 3, activation ='softmax',kernel_initializer=ru),\n",
    "    ])\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=(None,3)),\n",
    "                         tf.TensorSpec(shape=(None,408)),\n",
    "                         tf.TensorSpec(shape=(None,340)),])\n",
    "    def train_step(labels,hmm_data,pssm_data):\n",
    "        # forward pass on hmm1 neural network\n",
    "        with tf.GradientTape() as hmm_tape:\n",
    "            hmm_proba = hmm1_NN(hmm_data, training=True)\n",
    "            hmm_loss = loss_fn(labels, hmm_proba)\n",
    "\n",
    "        # forward pass on pssm1 neural network\n",
    "        with tf.GradientTape() as pssm_tape:\n",
    "            pssm_proba = pssm1_NN(pssm_data, training=True)\n",
    "            pssm_loss = loss_fn(labels, pssm_proba)\n",
    "\n",
    "        hmm_grads = hmm_tape.gradient(hmm_loss, hmm1_NN.trainable_weights)\n",
    "        pssm_grads = pssm_tape.gradient(pssm_loss, pssm1_NN.trainable_weights)\n",
    "\n",
    "        optimizer.apply_gradients(zip(hmm_grads, hmm1_NN.trainable_weights))\n",
    "        optimizer.apply_gradients(zip(pssm_grads, pssm1_NN.trainable_weights))\n",
    "        \n",
    "        return hmm_loss,pssm_loss\n",
    "\n",
    "     \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=(None,3)),\n",
    "                                  tf.TensorSpec(shape=(None,408)),\n",
    "                                  tf.TensorSpec(shape=(None,340)),])\n",
    "    def valid_step(labels,hmm_data, pssm_data):\n",
    "        hmm_proba = hmm1_NN(hmm_data, training=False)\n",
    "        pssm_proba = pssm1_NN(pssm_data, training=False)\n",
    "        \n",
    "        hmm_metric.update_state(labels, hmm_proba)\n",
    "        pssm_metric.update_state(labels, pssm_proba)\n",
    "        \n",
    "        # return validation loss for early stopping\n",
    "        hmm_loss = loss_fn(labels, hmm_proba)\n",
    "        pssm_loss = loss_fn(labels, pssm_proba)\n",
    "        return hmm_loss, pssm_loss\n",
    "    \n",
    "    # print to log.txt\n",
    "    def print(*args,**kwargs):\n",
    "        with open(fold_dir + 'log.txt','a') as f:\n",
    "            kwargs['file']=f\n",
    "            return builtins.print(*args,**kwargs)\n",
    "    \n",
    "    print('Training sequence to structure networks...')\n",
    "    hmm_valid_loss = [] # for early stopping\n",
    "    pssm_valid_loss = []\n",
    "    hmm_training_finished = False\n",
    "    pssm_training_finished = False\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        epoch_hmm_Tloss = 0\n",
    "        epoch_pssm_Tloss = 0\n",
    "        train_seqIDs = list(train_data.keys())\n",
    "        random.shuffle(train_seqIDs)\n",
    "        for step, seqID in enumerate(train_seqIDs):\n",
    "            \n",
    "            batch_size = len(train_data[seqID])\n",
    "            labels = train_data[seqID][:,0:3]\n",
    "            hmm_data = train_data[seqID][:,3:411]\n",
    "            pssm_data = train_data[seqID][:,411:751]\n",
    "            labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "            hmm_data = tf.convert_to_tensor(hmm_data, dtype=tf.float32)\n",
    "            pssm_data = tf.convert_to_tensor(pssm_data, dtype=tf.float32)\n",
    "            \n",
    "            hmm_loss, pssm_loss = train_step(labels,hmm_data,pssm_data)\n",
    "            \n",
    "            epoch_hmm_Tloss += hmm_loss/batch_size\n",
    "            epoch_pssm_Tloss += pssm_loss/batch_size\n",
    "        \n",
    "        # calculate training accuracy\n",
    "        for step, seqID in enumerate(train_seqIDs):\n",
    "            batch_size = len(train_data[seqID])\n",
    "            labels = train_data[seqID][:,0:3]\n",
    "            hmm_data = train_data[seqID][:,3:411]\n",
    "            pssm_data = train_data[seqID][:,411:751]\n",
    "            labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "            hmm_data = tf.convert_to_tensor(hmm_data, dtype=tf.float32)\n",
    "            pssm_data = tf.convert_to_tensor(pssm_data, dtype=tf.float32)\n",
    "            \n",
    "            valid_step(labels,hmm_data,pssm_data)\n",
    "        \n",
    "        hmm_acc_train = hmm_metric.result()\n",
    "        pssm_acc_train = pssm_metric.result()\n",
    "        hmm_metric.reset_states()\n",
    "        pssm_metric.reset_states()\n",
    "        \n",
    "        # end of epoch validation\n",
    "        epoch_hmm_loss = 0\n",
    "        epoch_pssm_loss = 0\n",
    "        valid_seqIDs = list(valid_data.keys())\n",
    "        random.shuffle(valid_seqIDs)\n",
    "        for step, seqID in enumerate(valid_seqIDs):\n",
    "            batch_size = len(valid_data[seqID])\n",
    "            labels = valid_data[seqID][:,:3]\n",
    "            hmm_data = valid_data[seqID][:,3:411]\n",
    "            pssm_data = valid_data[seqID][:,411:751]\n",
    "            labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "            hmm_data = tf.convert_to_tensor(hmm_data, dtype=tf.float32)\n",
    "            pssm_data = tf.convert_to_tensor(pssm_data, dtype=tf.float32)\n",
    "            \n",
    "            hmm_loss, pssm_loss = valid_step(labels,hmm_data,pssm_data)\n",
    "            \n",
    "            epoch_hmm_loss += hmm_loss/batch_size\n",
    "            epoch_pssm_loss += pssm_loss/batch_size\n",
    "        \n",
    "        hmm_valid_loss.append(epoch_hmm_loss)\n",
    "        pssm_valid_loss.append(epoch_pssm_loss)\n",
    "        hmm_acc_valid = hmm_metric.result()\n",
    "        pssm_acc_valid = pssm_metric.result()\n",
    "        hmm_metric.reset_states()\n",
    "        pssm_metric.reset_states()\n",
    "        \n",
    "        print(\n",
    "            \"Epoch %d HMM_acc %.4f PSSM_acc %.4f HMM_loss %.4f PSSM_loss %.4f HMM_Tacc %.4f PSSM_Tacc %.4f HMM_Tloss %.4f PSSM_Tloss %.4f\" \n",
    "            % (\n",
    "            epoch, hmm_acc_valid, pssm_acc_valid, epoch_hmm_loss, epoch_pssm_loss,\n",
    "                hmm_acc_train, pssm_acc_train, epoch_hmm_Tloss, epoch_pssm_Tloss\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # check for early stopping\n",
    "        hmm_stopEarly = Callback_EarlyStopping(hmm_valid_loss, min_delta=md, patience=20)\n",
    "        pssm_stopEarly = Callback_EarlyStopping(pssm_valid_loss, min_delta=md, patience=20)\n",
    "        \n",
    "        if not hmm_training_finished:\n",
    "            if hmm_stopEarly:\n",
    "                print(\"Early stopping for hmm1_NN at epoch %d/%d\" % (epoch,epochs))\n",
    "                hmm1_NN.save(fold_dir + 'hmm1_NN')\n",
    "                hmm_training_finished = True\n",
    "        \n",
    "        if not pssm_training_finished:\n",
    "            if pssm_stopEarly:\n",
    "                print(\"Early stopping for pssm1_NN at epoch %d/%d\" % (epoch,epochs))\n",
    "                pssm1_NN.save(fold_dir + 'pssm1_NN')\n",
    "                pssm_training_finished = True\n",
    "        \n",
    "        if hmm_training_finished and pssm_training_finished:\n",
    "            print(\"Training finished at epoch %d/%d\" % (epoch,epochs))\n",
    "            break\n",
    "            \n",
    "    print(\"Training finished at epoch %d\" % epochs)\n",
    "    if hmm_training_finished:\n",
    "        hmm1_NN.save(fold_dir + 'hmm1_NN_%d' % (epochs))\n",
    "    else:\n",
    "        hmm1_NN.save(fold_dir + 'hmm1_NN')\n",
    "    if pssm_training_finished:\n",
    "        pssm1_NN.save(fold_dir + 'pssm1_NN_%d' % (epochs))\n",
    "    else:\n",
    "        pssm1_NN.save(fold_dir + 'pssm1_NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab084466-98df-4403-9ea4-7c255bc02574",
   "metadata": {},
   "source": [
    "### Callback fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "178d7b5d-690a-4f0d-882d-3a82937ad3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/59438904/applying-callbacks-in-a-custom-training-loop-in-tensorflow-2-0\n",
    "# Matthew Thomas\n",
    "# keras implementation of early stopping\n",
    "# def Callback_EarlyStopping(MetricList, min_delta=0.1, patience=20, mode='min'):\n",
    "#     #No early stopping for the first patience epochs \n",
    "#     if len(MetricList) <= patience:\n",
    "#         return False\n",
    "    \n",
    "#     min_delta = abs(min_delta)\n",
    "#     if mode == 'min':\n",
    "#         min_delta *= -1\n",
    "#     else:\n",
    "#         min_delta *= 1\n",
    "    \n",
    "#     #last patience epochs \n",
    "#     last_patience_epochs = [x + min_delta for x in MetricList[::-1][1:patience + 1]]\n",
    "#     current_metric = MetricList[::-1][0]\n",
    "    \n",
    "#     if mode == 'min':\n",
    "#         return current_metric >= max(last_patience_epochs)\n",
    "#     else:\n",
    "#         return current_metric <= min(last_patience_epochs)\n",
    "\n",
    "def Callback_EarlyStopping(LossList, min_delta=0.1, patience=20):\n",
    "    #No early stopping for 2*patience epochs \n",
    "    if len(LossList)//patience < 2 :\n",
    "        return False\n",
    "    #Mean loss for last patience epochs and second-last patience epochs\n",
    "    mean_previous = np.mean(LossList[::-1][patience:2*patience]) #second-last\n",
    "    mean_recent = np.mean(LossList[::-1][:patience]) #last\n",
    "    #you can use relative or absolute change\n",
    "    delta_abs = np.abs(mean_recent - mean_previous) #abs change\n",
    "    delta_abs = np.abs(delta_abs / mean_previous)  # relative change\n",
    "    return delta_abs < min_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64975eca-897b-4468-bb06-c6ba1ce807aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8582655b-b310-4b26-ac0e-2c1ffe232cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_check():\n",
    "    # not actually needed but it does a sanity check on input data\n",
    "    dssp_files = glob.glob(data_dir + '*.dssp')\n",
    "    hmm_files = glob.glob(data_dir + '*.hmm')\n",
    "    pssm_files = glob.glob(data_dir + '*.pssm')\n",
    "    seq_files = glob.glob(data_dir + '*.fasta')\n",
    "    seqIDs = [f.split('/')[-1][:-5] for f in dssp_files]\n",
    "    set_seqIDs_all = set([f.split('/')[-1][:-6] for f in seq_files])\n",
    "    set_seqIDs = set(seqIDs)\n",
    "    unused_seqIDs = set_seqIDs ^ set_seqIDs_all # 9 of them are unused\n",
    "    seq_files = [f for f in seq_files if f.split('/')[-1][:-6] not in unused_seqIDs]\n",
    "    assert all([len(x)==1348 for x in [dssp_files,hmm_files,pssm_files,seq_files,seqIDs]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea203c9b-bddb-47f7-92f9-72e14c7129f9",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cd3efa7-bb8d-4c43-b517-0f265e23ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def generate_data_for_NN2(set_of_seqID,fold_dir):\n",
    "    hmm1_NN = keras.models.load_model(fold_dir+'hmm1_NN')\n",
    "    pssm1_NN = keras.models.load_model(fold_dir+'pssm1_NN')\n",
    "    data_in = generate_data(set_of_seqID)\n",
    "    \n",
    "    def process_seqID(data):\n",
    "        label = data[:,:3]\n",
    "        hmm1_in = data[:,3:411]\n",
    "        pssm1_in = data[:,411:751]\n",
    "        hmm1_out = hmm1_NN(hmm1_in, training=False).numpy()\n",
    "        pssm1_out = pssm1_NN(pssm1_in, training=False).numpy()\n",
    "        hmm2_in = sliding_window(hmm1_out, flank=9)\n",
    "        pssm2_in = sliding_window(pssm1_out, flank=9)\n",
    "        assert hmm2_in.shape[1]==57\n",
    "        assert pssm2_in.shape[1]==57\n",
    "        result = np.concatenate([label,hmm2_in,pssm2_in],axis=1)\n",
    "        return result\n",
    "    \n",
    "    arr_dict = {}\n",
    "    start_time = time.time()\n",
    "    for seqID in set_of_seqID:\n",
    "        arr_dict[seqID] = process_seqID(data_in[seqID])\n",
    "    print(\"Took %s seconds to process %d seqIDs\" % (time.time() - start_time, len(set_of_seqID)))\n",
    "    return arr_dict\n",
    "        \n",
    "# run this function to process multiple IDs\n",
    "def generate_data(seqIDs):\n",
    "    arr_list = Parallel(n_jobs=-1,verbose=0)(delayed(process_seqID)(seqID) for seqID in seqIDs)\n",
    "    return {seqID: arr for seqID, arr in zip(seqIDs,arr_list)}\n",
    "\n",
    "# get splits from resume.log generated by Perl shuffling scripts \n",
    "# returns sets of strings of seqIDs \n",
    "@memory.cache\n",
    "def get_splits(resume_log_file):\n",
    "    val_splits = []\n",
    "    set_idx = -1\n",
    "    cur_set = set() \n",
    "    with open(resume_log_file,'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        for line in lines:\n",
    "            if line.startswith('#SET'):\n",
    "                if set_idx > -1:\n",
    "                    val_splits.append(cur_set)\n",
    "                    cur_set = set()\n",
    "                set_idx += 1\n",
    "            else:\n",
    "                seqID = line.split('/')[-1].replace('.pssm','')\n",
    "                cur_set.add(seqID)\n",
    "        # append last set which is not followed by another line '#SET...'\n",
    "        val_splits.append(cur_set)\n",
    "    assert sum([len(s) for s in val_splits])==1348\n",
    "    return val_splits\n",
    "\n",
    "# produces a single numpy array for each sequence\n",
    "@memory.cache\n",
    "def process_seqID(seqID):\n",
    "    data_dir = '/cluster/gjb_lab/2472402/data/retr231_raw_files/training/'\n",
    "    hmm_path = data_dir + seqID + '.hmm'\n",
    "    pssm_path = data_dir + seqID + '.pssm'\n",
    "    dssp_path = data_dir + seqID + '.dssp'\n",
    "    assert os.path.exists(pssm_path)\n",
    "    hmm = np.loadtxt(hmm_path,delimiter=' ')\n",
    "    hmm = sliding_window(hmm,flank=8)\n",
    "    pssm = np.loadtxt(pssm_path,delimiter=' ')\n",
    "    pssm = sliding_window(pssm,flank=8)\n",
    "    dssp = get_dssp(dssp_path)\n",
    "    res = np.concatenate([dssp,hmm,pssm],axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc9a4c0-2056-4fcc-9757-5a7872a4ae25",
   "metadata": {},
   "source": [
    "### Preprocessing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24ac9a75-7683-44f4-b31b-fe03962d1283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in: np array. out: np array linearized over sliding window\n",
    "def sliding_window(array, flank):\n",
    "    assert flank > 0\n",
    "    assert type(array) is np.ndarray\n",
    "    assert np.logical_not(np.isnan(np.sum(array)))\n",
    "    nrow = array.shape[0]\n",
    "    assert nrow > 0\n",
    "    ncol = array.shape[1]\n",
    "    assert ncol > 0\n",
    "    res = np.empty(shape=(nrow, (2*flank+1)*ncol),dtype=np.float32)\n",
    "    res[:] = np.nan\n",
    "    for i in list(range(0,nrow)):\n",
    "        s, e = i-flank, i+flank+1\n",
    "        k = 0;\n",
    "        for j in list(range(s,e)):\n",
    "            if (j < 0 or j >= nrow):\n",
    "                res[i, k:k+ncol] = 0\n",
    "            else:\n",
    "                assert np.logical_not(np.isnan(np.sum(array[j])))\n",
    "                assert array[j].shape == (ncol,)\n",
    "                res[i, k:k+ncol] = array[j]\n",
    "            k += ncol\n",
    "    assert np.logical_not(np.isnan(np.sum(res)))\n",
    "    assert res.shape == (nrow, (2*flank+1)*ncol)\n",
    "    return res\n",
    "\n",
    "def encode(s):\n",
    "    res = np.empty(shape=(len(s),3),dtype=np.byte)\n",
    "    res[:] = np.nan\n",
    "    for i in range(0,len(s)):\n",
    "        if s[i] == 'H':\n",
    "            res[i] = np.array([0,1,0])\n",
    "        else:\n",
    "            if s[i] == 'E':\n",
    "                res[i] = np.array([1,0,0])\n",
    "            else:\n",
    "                assert s[i]\n",
    "                res[i] = np.array([0,0,1])\n",
    "    assert not np.isnan(np.sum(res))\n",
    "    return res\n",
    "\n",
    "def get_dssp(dssp_path):\n",
    "    with open(dssp_path,'r') as f:\n",
    "        dssp = f.read().strip()\n",
    "    return encode(dssp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c2eb8-cd4b-4405-ad53-8ce1acd297ba",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f434d8bf-de79-4117-8e9f-aa599e2877fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_train_valid(val_fold):\n",
    "    valid_set = val_splits[val_fold]\n",
    "    train_set = set().union(*(val_splits[:val_fold] + val_splits[val_fold+1:]))\n",
    "    return train_set, valid_set\n",
    "\n",
    "# main cross validation function here\n",
    "def run_cross_validation(val_splits):\n",
    "    \n",
    "    def split_train_valid(val_fold):\n",
    "        valid_set = val_splits[val_fold]\n",
    "        train_set = set().union(*(val_splits[:val_fold] + val_splits[val_fold+1:]))\n",
    "        return train_set, valid_set    \n",
    "    \n",
    "    def train_fold(fold):\n",
    "        train_set, valid_set = split_train_valid(fold)\n",
    "        fold_dir = './18-09-0/%s/' % fold\n",
    "        if not os.path.exists(fold_dir):\n",
    "            os.system('mkdir -p ' + fold_dir)\n",
    "            \n",
    "        seq_NN_params = {\n",
    "            'fold_dir' : fold_dir,\n",
    "            'learning_rate':1e-2,\n",
    "            'min_delta':1e-3,\n",
    "            'epochs':1,\n",
    "        }\n",
    "        struct_NN_params = {\n",
    "            'fold_dir' : fold_dir,\n",
    "            'learning_rate':1e-2,\n",
    "            'min_delta':1e-3,\n",
    "            'epochs':1,\n",
    "        }\n",
    "        train_seq_networks(train_set, valid_set, **seq_NN_params)\n",
    "        train_struct_networks(train_set, valid_set, **struct_NN_params)\n",
    "    \n",
    "    #return Parallel(n_jobs=-1,verbose=0)(delayed(train_fold)(fold) for fold in range(7))\n",
    "    \n",
    "    for fold in range(7):\n",
    "        train_fold(fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149bfea8-ccd8-4f0d-b3ea-5a3aaf4be56c",
   "metadata": {},
   "source": [
    "## Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77eb33fa-0b6d-45bf-a776-7635029b8b99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/234711.1.all.q/ipykernel_4945/2509517397.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mval_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cluster/gjb_lab/2472402/data/retr231_shuffles/shuffle02/best_shuffle_th_1.log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/234711.1.all.q/ipykernel_4945/3639789153.py\u001b[0m in \u001b[0;36mrun_cross_validation\u001b[0;34m(val_splits)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtrain_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/234711.1.all.q/ipykernel_4945/3639789153.py\u001b[0m in \u001b[0;36mtrain_fold\u001b[0;34m(fold)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;34m'epochs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         }\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mtrain_seq_networks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mseq_NN_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mtrain_struct_networks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mstruct_NN_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/234711.1.all.q/ipykernel_4945/1532569440.py\u001b[0m in \u001b[0;36mtrain_seq_networks\u001b[0;34m(train_idx_set, valid_idx_set, **params)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_idx_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_idx_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/234711.1.all.q/ipykernel_4945/2809512373.py\u001b[0m in \u001b[0;36mgenerate_data\u001b[0;34m(seqIDs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# run this function to process multiple IDs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqIDs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0marr_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_seqID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqID\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseqID\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqIDs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mseqID\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseqID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqIDs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marr_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_effective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_initialize_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;34m\"\"\"Build a process or thread pool and return the number of workers\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m             n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n\u001b[0m\u001b[1;32m    734\u001b[0m                                              **self._backend_args)\n\u001b[1;32m    735\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_timeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mconfigure\u001b[0;34m(self, n_jobs, parallel, prefer, require, idle_worker_timeout, **memmappingexecutor_args)\u001b[0m\n\u001b[1;32m    492\u001b[0m                 SequentialBackend(nesting_level=self.nesting_level))\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         self._workers = get_memmapping_executor(\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midle_worker_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_worker_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/joblib/executor.py\u001b[0m in \u001b[0;36mget_memmapping_executor\u001b[0;34m(n_jobs, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_memmapping_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mMemmappingExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_memmapping_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/joblib/executor.py\u001b[0m in \u001b[0;36mget_memmapping_executor\u001b[0;34m(cls, n_jobs, timeout, initializer, initargs, env, temp_folder, context_id, **backend_args)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mtemp_folder_resolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_temp_folder_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             **backend_args)\n\u001b[0;32m---> 52\u001b[0;31m         _executor, executor_is_reused = super().get_reusable_executor(\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_reducers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_reducers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_reducers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult_reducers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/joblib/externals/loky/reusable_executor.py\u001b[0m in \u001b[0;36mget_reusable_executor\u001b[0;34m(cls, max_workers, context, timeout, kill_workers, reuse, job_reducers, result_reducers, initializer, initargs, env)\u001b[0m\n\u001b[1;32m    158\u001b[0m                         \u001b[0;34m\"previous instance cannot be reused ({}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                         .format(max_workers, reason))\n\u001b[0;32m--> 160\u001b[0;31m                     \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                     \u001b[0m_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_executor_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                     \u001b[0;31m# Recursive call to build a new instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/ml-env/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexecutor_manager_thread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m             \u001b[0mexecutor_manager_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0;31m# To reduce the risk of opening too many files, remove references to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/miniconda/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/gjb_lab/2472402/miniconda/lib/python3.9/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_splits = get_splits('/cluster/gjb_lab/2472402/data/retr231_shuffles/shuffle02/best_shuffle_th_1.log')\n",
    "run_cross_validation(val_splits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ml-env)",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

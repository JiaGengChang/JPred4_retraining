{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889cf9e6-e4e7-4381-9f90-8ad951b478b1",
   "metadata": {},
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e25f681-b956-4e0a-9cf3-aefc50aa5125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-19 17:15:26.951778: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from joblib import Parallel,delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f60ad30-766b-425f-b1a2-3a6c55e1decc",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c718f0ce-6c7c-4ce5-b6aa-761e359917a7",
   "metadata": {},
   "source": [
    "## Helpers - Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb7d8589-66a1-4a8c-999e-d1010fc4e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential sliding window\n",
    "# converts N by M into N by 2(flank+1)(M)\n",
    "def window(matin, flank):\n",
    "#     print(matin.shape)\n",
    "    nrow = matin.shape[0]\n",
    "    ncol = matin.shape[1]\n",
    "    matout = np.zeros(shape=(nrow, (2*flank+1)*ncol), dtype=np.float32)\n",
    "    for i in list(range(0,nrow)):\n",
    "        s, e = i-flank, i+flank+1\n",
    "        k = 0;\n",
    "        for j in list(range(s,e)):\n",
    "            if (j > 0 and j < nrow):\n",
    "                matout[i,k:k+ncol] = matin[j]\n",
    "            k += ncol\n",
    "    return tf.convert_to_tensor(matout)\n",
    "\n",
    "# one hot encoding\n",
    "# e.g. input:\n",
    "# -H---HHHH--EE--\n",
    "# will output:\n",
    "# [[0,0,1],[0,1,0],[0,0,1],...]\n",
    "def encode_dssp(s):\n",
    "    res = np.empty(shape=(len(s),3),dtype=np.byte)\n",
    "    res[:] = np.nan\n",
    "    for i in range(0,len(s)):\n",
    "        if s[i] == 'H':\n",
    "            res[i] = np.array([0,1,0])\n",
    "        else:\n",
    "            if s[i] == 'E':\n",
    "                res[i] = np.array([1,0,0])\n",
    "            else:\n",
    "                assert s[i]\n",
    "                res[i] = np.array([0,0,1])\n",
    "    assert not np.isnan(np.sum(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51b6062-f4e9-43c3-9198-d6d8d9109177",
   "metadata": {},
   "source": [
    "## Helpers - Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ebcbd4-d803-40ad-81d1-6ab976329c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_seqID(seqID):\n",
    "    data_dir = '/cluster/gjb_lab/2472402/data/retr231_raw_files/training/'\n",
    "    hmm_path = data_dir + seqID + '.hmm'\n",
    "    pssm_path = data_dir + seqID + '.pssm'\n",
    "    dssp_path = data_dir + seqID + '.dssp'\n",
    "    assert os.path.exists(pssm_path)\n",
    "    hmm = np.loadtxt(hmm_path,delimiter=' ',dtype=np.float32)\n",
    "    pssm = np.loadtxt(pssm_path,delimiter=' ',dtype=np.float32)\n",
    "    dssp = parse_dssp(dssp_path)\n",
    "    return [tf.convert_to_tensor(x,dtype=tf.float32) for x in [dssp,hmm,pssm]]\n",
    "\n",
    "def parse_dssp(dssp_path):\n",
    "    with open(dssp_path,'r') as f:\n",
    "        dssp = f.read().strip()\n",
    "    return encode_dssp(dssp)\n",
    "\n",
    "def parse_log_file(log_file_path):\n",
    "    val_splits = []\n",
    "    set_idx = -1\n",
    "    cur_set = set() \n",
    "    with open(log_file_path,'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        for line in lines:\n",
    "            if line.startswith('#SET'):\n",
    "                if set_idx > -1:\n",
    "                    val_splits.append(cur_set)\n",
    "                    cur_set = set()\n",
    "                set_idx += 1\n",
    "            else:\n",
    "                seqID = line.split('/')[-1].replace('.pssm','')\n",
    "                cur_set.add(seqID)\n",
    "        # append last set which is not followed by another line '#SET...'\n",
    "        val_splits.append(cur_set)\n",
    "    assert sum([len(s) for s in val_splits])==1348\n",
    "    return val_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe05c8b-ffc8-4962-bdfb-04651ee2429e",
   "metadata": {},
   "source": [
    "## Jnet classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eaace479-2921-42d1-8c0c-0e5cf85ee085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JnetClassifier(keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(JnetClassifier, self).__init__()\n",
    "        self.hmm1 = MLPBlock(408)\n",
    "        self.hmm2 = MLPBlock(57)\n",
    "        self.psi1 = MLPBlock(340)\n",
    "        self.psi2 = MLPBlock(57)\n",
    "    \n",
    "    def call(self, data):\n",
    "        \n",
    "        x1 = data[:,:24]\n",
    "        x1 = window(x1,flank=8)\n",
    "        x1 = self.hmm1(x1)\n",
    "        x1 = window(x1,flank=9)\n",
    "        x1 = self.hmm2(x1)\n",
    "        \n",
    "        x2 = data[:,24:44]\n",
    "        x2 = window(x2,flank=8)\n",
    "        x2 = self.psi1(x2)\n",
    "        x2 = window(x2,flank=9)\n",
    "        x2 = self.psi2(x2)\n",
    "        \n",
    "        return x1+x2//2\n",
    "   \n",
    "    def compile(self,loss,optimizer,metrics,**kwargs):\n",
    "        super(JnetClassifier, self).compile(**kwargs)\n",
    "        self.hmm1.compile(loss=loss,optimizer=optimizer,metrics=metrics)\n",
    "        self.hmm2.compile(loss=loss,optimizer=optimizer,metrics=metrics)\n",
    "        self.psi1.compile(loss=loss,optimizer=optimizer,metrics=metrics)\n",
    "        self.psi2.compile(loss=loss,optimizer=optimizer,metrics=metrics)\n",
    "    \n",
    "    def train_step(self,inputs):\n",
    "        \n",
    "        data,dssp = inputs\n",
    "        x1 = data[:,:24]\n",
    "        x2 = data[:,24:44]\n",
    "        assert x1.shape[0]==x2.shape[0]\n",
    "        \n",
    "        # window inputs\n",
    "        x11 = window(x1,flank=8)\n",
    "        x21 = window(x2,flank=8)\n",
    "        \n",
    "        # should transfer this to MLPBlock class\n",
    "        with tf.GradientTape() as tape:\n",
    "            hmm_pred1 = self.hmm1(x11)\n",
    "            hmm_loss1 = self.hmm1.compiled_loss(dssp,hmm_pred1)\n",
    "        grad = tape.gradient(hmm_loss1, self.hmm1.trainable_variables)\n",
    "        self.hmm1.optimizer.apply_gradients(zip(grad,self.hmm1.trainable_variables))\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            psi_pred1 = self.psi1(x21)\n",
    "            psi_loss1 = self.psi1.compiled_loss(dssp,psi_pred1)\n",
    "        grad = tape.gradient(psi_loss1, self.psi1.trainable_variables)\n",
    "        self.psi1.optimizer.apply_gradients(zip(grad,self.psi1.trainable_variables))\n",
    "        \n",
    "        # window inputs \n",
    "        x12 = window(hmm_pred1,flank=9)\n",
    "        x22 = window(psi_pred1,flank=9)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            hmm_pred2 = self.hmm2(x12)\n",
    "            hmm_loss2 = self.hmm2.compiled_loss(dssp,hmm_pred2)\n",
    "        grad = tape.gradient(hmm_loss2, self.hmm2.trainable_variables)\n",
    "        self.hmm2.optimizer.apply_gradients(zip(grad,self.hmm2.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            psi_pred2 = self.hmm2(x22)\n",
    "            psi_loss2 = self.psi2.compiled_loss(dssp,psi_pred2)\n",
    "        grad = tape.gradient(psi_loss2, self.psi2.trainable_variables)\n",
    "        self.psi2.optimizer.apply_gradients(zip(grad,self.psi2.trainable_variables))\n",
    "        \n",
    "        return {'hmm_loss1':hmm_loss1,'hmm_loss2':hmm_loss2,'pssm_loss1':psi_loss1,'pssm_loss2':psi_loss2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1613953d-d3ce-4170-bd92-04a9e7c2c5c8",
   "metadata": {},
   "source": [
    "## MLP block class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2fc516a-8cc3-4816-bf49-f38e38a203a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(keras.Sequential):\n",
    "    def __init__(self, input_shape):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.kernit = keras.initializers.RandomUniform(minval=-0.05, maxval=0.05)\n",
    "        self.layer1 = keras.layers.InputLayer(input_shape=[input_shape])\n",
    "        self.layer2 = keras.layers.Dense(units=100,\n",
    "                                         activation='sigmoid',\n",
    "                                         kernel_initializer=self.kernit)\n",
    "        self.layer3 = keras.layers.Dense(units=3,\n",
    "                                         activation='softmax',\n",
    "                                         kernel_initializer=self.kernit)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.layer1(inputs)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63580b88-e31f-460e-beec-b0c559c96fea",
   "metadata": {},
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c7fe5d6-e4a7-4363-9c8e-16f57b4bc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, dssp, hmm, pssm, batch_size, shuffle=True):\n",
    "        self.n = 0\n",
    "        self.dssp = dssp\n",
    "        self.hmm = hmm\n",
    "        self.pssm = pssm\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.dssp)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        startIdx = idx*self.batch_size\n",
    "        endIdx = (idx+1)*self.batch_size\n",
    "        batch_dssp = self.dssp[startIdx:endIdx]\n",
    "        batch_data = tf.concat([\n",
    "            self.hmm[startIdx:endIdx],\n",
    "            self.pssm[startIdx:endIdx]\n",
    "        ],axis=1)\n",
    "        return (batch_data,batch_dssp)\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.n >= self.__len__():\n",
    "            self.n = 0\n",
    "        idx = self.n\n",
    "        self.n += 1\n",
    "        return self.__getitem__(idx)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            idx = tf.range(start=0, limit=tf.shape(self.dssp)[0])\n",
    "            new_idx = tf.random.shuffle(idx)\n",
    "            self.hmm = tf.gather(self.hmm,new_idx)\n",
    "            self.pssm = tf.gather(self.pssm,new_idx)\n",
    "            self.dssp = tf.gather(self.dssp,new_idx)\n",
    "\n",
    "class ValidDataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    # variable batch sizes\n",
    "    # D is dictionary of key => [dssp,hmm,pssm]\n",
    "    def __init__(self, D):\n",
    "        self.D = D\n",
    "        self.len = len(D)\n",
    "        self.seqIDs = list(self.D.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        val = self.D[self.seqIDs[idx]]\n",
    "        batch_dssp = val[0]\n",
    "        batch_data = tf.concat([\n",
    "            val[1],val[2]\n",
    "        ],axis=1)\n",
    "        return (batch_data,batch_dssp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99b6c2-6d6d-426b-9662-bed474c7008d",
   "metadata": {},
   "source": [
    "## Data wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "825c2f8e-d405-4cb7-8613-20cf4282f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    log_file = '/cluster/gjb_lab/2472402/data/retr231_shuffles/shuffle02/best_shuffle_th_1.log'\n",
    "    splits = parse_log_file(log_file)\n",
    "    return Parallel(n_jobs=-1,verbose=0)(\n",
    "        delayed(\n",
    "            lambda split: {seqID:parse_seqID(seqID) for seqID in split}\n",
    "        ) (split_i) for split_i in splits\n",
    "    )\n",
    "\n",
    "def get_train_generator(d_train,batch_size):\n",
    "    dsspL, hmmL, pssmL = [],[],[]\n",
    "    for _, (dssp,hmm,pssm) in d_train.items():\n",
    "        dsspL.append(dssp)\n",
    "        hmmL.append(hmm)\n",
    "        pssmL.append(pssm)\n",
    "\n",
    "    dssp, hmm, pssm = [tf.concat(L,axis=0) for L in [dsspL,hmmL,pssmL]]\n",
    "    #shuffle\n",
    "    idx = tf.range(start=0, limit=tf.shape(dssp)[0])\n",
    "    shuffled_idx = tf.random.shuffle(idx)\n",
    "    dssp, hmm, pssm = [tf.gather(mat,shuffled_idx) for mat in [dssp,hmm,pssm]]\n",
    "    \n",
    "    return TrainDataGenerator(dssp,hmm,pssm,batch_size)\n",
    "\n",
    "def get_valid_generator(d_valid):\n",
    "    return ValidDataGenerator(d_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162842a3-1dd7-46e2-ac2c-846289142198",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89843f60-64cd-4a38-aca9-3f508eb20689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_val(DEBUG=False,**params):\n",
    "    \n",
    "    # load params\n",
    "    if DEBUG:\n",
    "        batch_size = 128\n",
    "        loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "        optim = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "        epochs = 10\n",
    "    else:\n",
    "        batch_size = params['batch_size']\n",
    "        loss_fn = params['loss_fn']\n",
    "        optim = params['optimizer']\n",
    "        epochs = params['epochs']\n",
    "    \n",
    "    splits = load_data()\n",
    "    \n",
    "    def split_data(val_idx):\n",
    "        d_train={}\n",
    "        for idx, d_split in enumerate(splits):\n",
    "            if idx!=val_idx:\n",
    "                d_train.update(d_split)\n",
    "        return d_train, splits[val_idx]\n",
    "    \n",
    "    krange = [4] if DEBUG else range(7)\n",
    "        \n",
    "    for k in krange:\n",
    "        jnet = JnetClassifier()\n",
    "        \n",
    "        jnet.compile(loss = loss_fn, optimizer = optim)\n",
    "        \n",
    "        d_train,d_valid = split_data(k)\n",
    "        train_generator = get_train_generator(d_train,batch_size)\n",
    "        valid_generator = get_valid_generator(d_valid)\n",
    "        \n",
    "        fit_params = {\n",
    "            'x' : train_generator,\n",
    "            'validation_data' : valid_generator,\n",
    "            'verbose' : 2,\n",
    "            'callbacks' : None,\n",
    "            'workers' : 4,\n",
    "            'use_multiprocessing' : True,\n",
    "            'epochs':epochs\n",
    "        }\n",
    "        \n",
    "        jnet.fit(**fit_params)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4cf9bb-ea6d-4635-929f-4e37eb092181",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dcc42d2-23b6-4b4b-893e-0666d8f50193",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-19 17:15:47.450123: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-09-19 17:15:47.508781: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-09-19 17:15:47.554582: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-09-19 17:15:47.757955: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-09-19 17:15:47.866470: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-09-19 17:15:47.917904: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-09-19 17:15:47.959730: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-09-19 17:15:48.852027: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-09-19 17:15:49.277579: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-09-19 17:15:49.277676: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c6320-6-1.compute.dundee.ac.uk): /proc/driver/nvidia/version does not exist\n",
      "2021-09-19 17:15:49.279474: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 431 ms, sys: 1.33 s, total: 1.76 s\n",
      "Wall time: 6.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "splits = load_data()\n",
    "def split_data(val_idx):\n",
    "    splits\n",
    "    d_train={}\n",
    "    for idx, d_split in enumerate(splits):\n",
    "        if idx!=val_idx:\n",
    "            d_train.update(d_split)\n",
    "    return d_train, splits[val_idx]\n",
    "d_train, d_valid = split_data(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a2d20-3497-4d6e-aa61-234943de24f1",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6995444-55ff-4edd-a77a-8cf5f25221b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "963ce306-26db-425e-ac1d-9b60f56f7f18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "8/8 [==============================] - 28s 3s/step - hmm_loss1: 1.1220 - hmm_loss2: 1.0838 - pssm_loss1: 1.0978 - pssm_loss2: 1.0840\n",
      "Epoch 2/2\n",
      "8/8 [==============================] - 28s 3s/step - hmm_loss1: 1.1216 - hmm_loss2: 1.0847 - pssm_loss1: 1.0974 - pssm_loss2: 1.0849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b4559996670>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_train_generator(d_train,batch_size):\n",
    "    dsspL, hmmL, pssmL = [],[],[]\n",
    "    for i, (dssp,hmm,pssm) in d_train.items():\n",
    "        if int(i) % 100 == 0:\n",
    "            dsspL.append(dssp)\n",
    "            hmmL.append(hmm)\n",
    "            pssmL.append(pssm)\n",
    "\n",
    "    dssp, hmm, pssm = [tf.concat(L,axis=0) for L in [dsspL,hmmL,pssmL]]\n",
    "    #shuffle\n",
    "    idx = tf.range(start=0, limit=tf.shape(dssp)[0])\n",
    "    shuffled_idx = tf.random.shuffle(idx)\n",
    "    dssp, hmm, pssm = [tf.gather(mat,shuffled_idx) for mat in [dssp,hmm,pssm]]\n",
    "    \n",
    "    return TrainDataGenerator(dssp,hmm,pssm,batch_size)\n",
    "\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "optim = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "jnet = JnetClassifier()\n",
    "d_train,d_valid = split_data(4)\n",
    "train_generator = get_train_generator(d_train,batch_size=256)\n",
    "valid_generator = get_valid_generator(d_valid)\n",
    "\n",
    "# works (run_eagerly=True)\n",
    "jnet.compile(loss = loss_fn, optimizer = optim, metrics = [],run_eagerly=True)\n",
    "jnet.fit(train_generator, epochs=2)\n",
    "\n",
    "# does not work (run_eagerly=False)\n",
    "# jnet.compile(loss = loss_fn, optimizer = optim, metrics = [],run_eagerly=False)\n",
    "# jnet.fit(train_generator, epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "827bf16b-1a09-4b07-a299-8a3aed3de2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "732/732 [==============================] - 3s 3ms/step - loss: 1.1016\n",
      "Epoch 2/2\n",
      "732/732 [==============================] - 2s 3ms/step - loss: 1.1016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ba8eb9f25b0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "optim = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "epochs = 10\n",
    "\n",
    "hmm1 = MLPBlock(408)\n",
    "hmm1.compile(loss = loss_fn, optimizer = optim)\n",
    "\n",
    "d_train,d_valid = split_data(4)\n",
    "train_generator = get_train_generator(d_train,batch_size)\n",
    "valid_generator = get_valid_generator(d_valid)\n",
    "\n",
    "fit_params = {\n",
    "    'data_generator' : train_generator,\n",
    "    'validation_data' : valid_generator,\n",
    "    'verbose' : 2,\n",
    "    'callbacks' : None,\n",
    "    'workers' : 4,\n",
    "    'use_multiprocessing' : True,\n",
    "    'epochs':epochs\n",
    "}\n",
    "\n",
    "hmm1.fit(train_generator, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18fbe028-ca6b-4444-9c6b-de51cb0a5085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "732/732 [==============================] - 2s 3ms/step - loss: 1.0996\n",
      "Epoch 2/2\n",
      "732/732 [==============================] - 2s 3ms/step - loss: 1.0996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2abee3a254f0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "optim = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "epochs = 10\n",
    "\n",
    "psi1 = MLPBlock(340)\n",
    "psi1.compile(loss = loss_fn, optimizer = optim)\n",
    "\n",
    "d_train,d_valid = split_data(4)\n",
    "train_generator = get_train_generator(d_train,batch_size)\n",
    "valid_generator = get_valid_generator(d_valid)\n",
    "\n",
    "fit_params = {\n",
    "    'data_generator' : train_generator,\n",
    "    'validation_data' : valid_generator,\n",
    "    'verbose' : 2,\n",
    "    'callbacks' : None,\n",
    "    'workers' : 4,\n",
    "    'use_multiprocessing' : True,\n",
    "    'epochs':epochs\n",
    "}\n",
    "\n",
    "psi1.fit(train_generator, epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ml-env)",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read, split, and process training data for cross-validation\n",
    "\n",
    "The purpose of this notebook is to parse training profiles as seven separate subsets for cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Import core modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Specify directory of dataset for training and testing\n",
    "Assumes that current working directory is '/homes/2472402/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = '/cluster/gjb_lab/2472402/retr231/training/'\n",
    "train_dirs = [train_dir + str(i) + '/' for i in range(1,8)]\n",
    "\n",
    "# nested list of file names with .fasta extension\n",
    "file_names = [os.listdir(train_dir) for train_dir in train_dirs]\n",
    "\n",
    "# remove .fasta extension to leave us with just the sequence IDs\n",
    "# do this with nested lsit comprehension\n",
    "seq_names = [[f[:-6] for f in file_name] for file_name in file_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Check that across the subsets, all names are unique, there are no repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1 is to pool all sequences into a single list\n",
    "seq_names_pooled = [seq for sublist in seq_names for seq in sublist]\n",
    "\n",
    "# convert to series\n",
    "seq_names_pooled_series = pd.Series(seq_names_pooled)\n",
    "\n",
    "# check that number of unique elements is equal to total number of elements\n",
    "# if true then pass\n",
    "seq_names_pooled_series.nunique() == len(seq_names_pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Convert the seq names to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cross_validation_subsets = {k:v for k,v in zip(list(range(1,8)), seq_names)}\n",
    "\n",
    "# save dictionary\n",
    "with open('cross_val_dict.pkl', 'wb') as fp:\n",
    "    pickle.dump(cross_validation_subsets, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Load in .profile files and split them based on cross validation dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create nested list of .profile file names to read\n",
    "profile_names = [[seq + '.profile' for seq in sublist] for sublist in seq_names ]\n",
    "\n",
    "profile_dir = '/cluster/gjb_lab/2472402/retr231/training/' # path to training dataset\n",
    "\n",
    "# create nested list of profiles, stored as dataframes\n",
    "# profiles_list = pd.read_csv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse profiles as nested list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Use the code from PSSM_parse.ipynb to parse in profiles in nested list, rather than a simple list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read all profiles (files with the .profile extension), collate into a list\n",
    "profiles_list = [[pd.read_csv(profile_dir + pn) for pn in sublist] for sublist in profile_names]\n",
    "\n",
    "# might want to run this in future if want to keep track of sequence_ID\n",
    "# profiles = [pn[:-8]: pd.read_csv(train_dir + pn) for pn in profile_names]\n",
    "\n",
    "# store column names of PSSM matrix as a constant\n",
    "column_names = ['A','R','N','D','C','Q','E','G','H','I','L','K','M','F','P','S','T','W','Y','V']\n",
    "\n",
    "def logistic_func(x):\n",
    "    x = int(x) # cast string to int\n",
    "    return 1/(1+math.exp(-1*x)) # simple case of logistic function\n",
    "\n",
    "def process_profile(pf):\n",
    "    # for each list element, take only the pssm column\n",
    "    pf = pf['Last position-specific scoring matrix computed'] \n",
    "    # for each list element, remove the last 5 rows from each pssm profile, i.e. k and lambda metrics\n",
    "    pf = pf[:-5]\n",
    "    # for each list element, split each row using default whitespace delimiter. take only elements 0 - 20, i.e. the 20 amino acid residues\n",
    "    pf = pd.DataFrame([row.split() for row in pf])\n",
    "    # obtain sequence in list form - this will become the row names\n",
    "    index = pf.iloc[1:,1].tolist()\n",
    "    # trim away unneeded portion of dataframe, including numerical index, and cols 23 onwards\n",
    "    pf = pf.iloc[1:, 2:22]\n",
    "    # assign column names (the same for all PSSM profiles)\n",
    "    pf.columns = column_names\n",
    "    # assign row names of pf\n",
    "    pf.index = index\n",
    "    return pf\n",
    "\n",
    "# perform these steps at one go, one profile at a time\n",
    "profiles_list = [[process_profile(pf) for pf in sublist] for sublist in profiles_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Squash using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pf_list = [[pf.applymap(logistic_func) for pf in sublist] for sublist in profiles_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Label with dssp truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>R</th>\n",
       "      <th>N</th>\n",
       "      <th>D</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>E</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>...</th>\n",
       "      <th>K</th>\n",
       "      <th>M</th>\n",
       "      <th>F</th>\n",
       "      <th>P</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>W</th>\n",
       "      <th>Y</th>\n",
       "      <th>V</th>\n",
       "      <th>dssp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.731059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.997527</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.731059</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.880797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.731059</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.993307</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.999665</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.880797</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.993307</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.731059</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.731059</td>\n",
       "      <td>0.731059</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.952574</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.997527</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.997527</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.999089</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.952574</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.952574</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>0.952574</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.993307</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.731059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.993307</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.731059</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.880797</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.999089</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.119203</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           A         R         N         D         C         Q         E  \\\n",
       "M   0.268941  0.268941  0.119203  0.047426  0.119203  0.500000  0.119203   \n",
       "P   0.268941  0.047426  0.047426  0.119203  0.017986  0.119203  0.119203   \n",
       "L   0.268941  0.047426  0.017986  0.006693  0.119203  0.047426  0.017986   \n",
       "P   0.119203  0.047426  0.047426  0.047426  0.017986  0.119203  0.119203   \n",
       "A   0.993307  0.047426  0.119203  0.731059  0.119203  0.119203  0.119203   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "D   0.017986  0.047426  0.119203  0.997527  0.002473  0.268941  0.997527   \n",
       "R   0.047426  0.999089  0.119203  0.017986  0.002473  0.952574  0.119203   \n",
       "G   0.952574  0.047426  0.119203  0.047426  0.047426  0.047426  0.047426   \n",
       "A   0.993307  0.119203  0.119203  0.047426  0.268941  0.119203  0.119203   \n",
       "G   0.500000  0.047426  0.268941  0.119203  0.047426  0.119203  0.047426   \n",
       "\n",
       "           G         H         I  ...         K         M         F         P  \\\n",
       "M   0.047426  0.119203  0.731059  ...  0.268941  0.997527  0.500000  0.047426   \n",
       "P   0.047426  0.047426  0.047426  ...  0.268941  0.047426  0.017986  0.999877   \n",
       "L   0.006693  0.017986  0.880797  ...  0.047426  0.731059  0.268941  0.017986   \n",
       "P   0.047426  0.047426  0.017986  ...  0.119203  0.047426  0.006693  0.999665   \n",
       "A   0.119203  0.047426  0.268941  ...  0.119203  0.119203  0.017986  0.047426   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "D   0.017986  0.047426  0.002473  ...  0.119203  0.006693  0.002473  0.017986   \n",
       "R   0.017986  0.500000  0.002473  ...  0.952574  0.017986  0.006693  0.017986   \n",
       "G   0.993307  0.047426  0.731059  ...  0.047426  0.047426  0.017986  0.047426   \n",
       "A   0.268941  0.047426  0.268941  ...  0.119203  0.268941  0.047426  0.119203   \n",
       "G   0.999089  0.047426  0.006693  ...  0.119203  0.017986  0.017986  0.047426   \n",
       "\n",
       "           S         T         W         Y         V  dssp  \n",
       "M   0.119203  0.268941  0.119203  0.268941  0.731059     -  \n",
       "P   0.268941  0.119203  0.006693  0.017986  0.047426     -  \n",
       "L   0.047426  0.268941  0.047426  0.119203  0.993307     -  \n",
       "P   0.268941  0.880797  0.006693  0.017986  0.047426     -  \n",
       "A   0.731059  0.731059  0.006693  0.047426  0.952574     -  \n",
       "..       ...       ...       ...       ...       ...   ...  \n",
       "D   0.119203  0.047426  0.002473  0.006693  0.006693     H  \n",
       "R   0.268941  0.047426  0.006693  0.017986  0.006693     -  \n",
       "G   0.268941  0.119203  0.017986  0.017986  0.119203     -  \n",
       "A   0.731059  0.268941  0.017986  0.047426  0.880797     -  \n",
       "G   0.268941  0.119203  0.047426  0.017986  0.017986     -  \n",
       "\n",
       "[270 rows x 21 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load DSSP information for that sequence\n",
    "dssps = [[pd.read_csv(profile_dir + sequence_ID + '.dssp') for sequence_ID in sublist] for sublist in seq_names]\n",
    "\n",
    "# extract out the string from the dataframe\n",
    "dssps = [[dssp.iloc[0,0] for dssp in sublist] for sublist in dssps]\n",
    "\n",
    "# convert string into list of 'H', 'E' or '-' characters, 1 for each residue\n",
    "dssps = [[[char for char in dssp] for dssp in sublist] for sublist in dssps]\n",
    "\n",
    "def add_dssp(profile, dssp):\n",
    "    profile['dssp'] = dssp\n",
    "    return profile\n",
    "\n",
    "labelled_pfs = [[add_dssp(pf, dssp) for (pf, dssp) in zip(pf_list_sub, dssps_sub)] for (pf_list_sub, dssps_sub) in zip(pf_list, dssps)]\n",
    "\n",
    "#see an example\n",
    "labelled_pfs[4][192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Save as pickle object (in case we want to use it for 2D CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/homes/2472402/data/pssm-sdx.pkl', 'wb') as fp:\n",
    "    pickle.dump(labelled_pfs, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# s = squashed, d = labelled with dssp information, x = split into groups for cross-validation, w = windowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding window operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Define some functions for the operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to add padding of #FLANK rows to the top and bottom of dataframe. flank = 8 for layer 1 (seq to struct) and flank = 9 for layer 2 (struct to struct)\n",
    "def padding_operation(profile, flank):\n",
    "    # create a numpy array with 21 NaNs\n",
    "    pad_np_array = np.hstack((np.repeat(np.nan,21)))\n",
    "    # convert numpy array into pd Series so we can create dataframe based on it\n",
    "    pad_row = pd.Series(pad_np_array)\n",
    "    # create empty dataframe and add the NaN rows to it\n",
    "    pad_df = pd.DataFrame()\n",
    "    for _ in range(flank):\n",
    "        pad_df = pad_df.append(pad_row, ignore_index=True)\n",
    "    # assign column names to this padding df - same column names as ppsm matrix\n",
    "    pad_df.columns = profile.columns\n",
    "    # create a copy of profile which i will now modify\n",
    "    pf = profile.copy()\n",
    "    # reset index names from pf, otherwise pd.concat (next line) will throw error saying index has to be nonredundant\n",
    "    pf.index = range(0,len(pf.index))\n",
    "    # add padding to above and below dataframes\n",
    "    padded_pf = pd.concat([pad_df, pf, pad_df], ignore_index=True)\n",
    "    return padded_pf\n",
    "\n",
    "# function returns a list of windows (variable name patterns). flank argument is same as above. takes in the padded_df from padding_operation()\n",
    "def sliding_operation(padded_pf, flank):\n",
    "    # Using iloc based indexing. i.e. 'location based indexing' cf label based indexing\n",
    "    # idx of first amino acid of sequence in padded dataframe \n",
    "    seqStartIdx = flank \n",
    "    # idx of last amino acid of sequence in padded dataframe\n",
    "    seqEndIdx = padded_pf.shape[0]-flank\n",
    "    # create a list of dataframes which we will put individual windows into. this list of patterns should span across samples\n",
    "    patterns = []\n",
    "    for seqIdx in range(seqStartIdx, seqEndIdx): # for each amino acid sequence / centroid of window\n",
    "        # define start and end index of window given position of center residue\n",
    "        winStartIdx = seqIdx - 8\n",
    "        winEndIdx = seqIdx + 8 + 1\n",
    "        # again using location based indexing with .iloc[]\n",
    "        pattern = padded_pf.iloc[winStartIdx:winEndIdx, :]\n",
    "        patterns.append(pattern)\n",
    "    return patterns\n",
    "\n",
    "# Wrapper function that generates N patterns from an unpadded profile\n",
    "def get_patterns(profile):\n",
    "    # add padding\n",
    "    padded_pf = padding_operation(profile, flank = 8)\n",
    "    # genereate windows\n",
    "    patterns = sliding_operation(padded_pf, flank = 8)\n",
    "    return(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Generate patterns for each profile in each training subset, then collapse all the patterns in each training subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a doubly nested list consisting of 7 elements, each element containing 193 subelements that represent individual profiles.  \n",
    "# these subelements are themselves lists, each containing N profiles where N is sequence length/number of rows in that profile\n",
    "pats_nested = [[get_patterns(pf) for pf in pfs] for pfs in labelled_pfs]\n",
    "pats_collapsed = [[pat for pats_sub in pats for pat in pats_sub] for pats in pats_nested]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Save patterns as pkl object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/homes/2472402/data/pssm-sdxw.pkl', 'wb') as fp:\n",
    "    pickle.dump(pats_collapsed, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# s = squashed, d = labelled with dssp information, x = split into groups for cross-validation, w = windowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearize patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Define functions to linearize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to collapse rows in df into a single row dataframe. \n",
    "# this is a inner function, it should not be applied on patterns containing dssp information\n",
    "def _linearize(df):\n",
    "    ndf = df.T.unstack().to_frame().T\n",
    "    ndf.columns = ndf.columns.swaplevel()\n",
    "    return ndf\n",
    "\n",
    "# wrapper function of _linearize\n",
    "# apply this on patterns containing dssp information\n",
    "# it will return a single-row df with dssp information as the last column\n",
    "def linearize(labelled_pat):\n",
    "    dssp_col = labelled_pat.iloc[:,-1].tolist() # store dssp column as a list\n",
    "    centroid_idx = int(len(dssp_col)/2) # index of centre of window\n",
    "    target = dssp_col[centroid_idx] # store prediction target. will be H, E, or -\n",
    "    unlabelled_pat = labelled_pat.iloc[:, :-1] # remove dssp column\n",
    "    unlabelled_pat.reset_index(drop=True, inplace = True) # reset index so that column name will be same across all patterns\n",
    "    linearized_pat = _linearize(unlabelled_pat) # convert unlabelled pattern into a single-row df\n",
    "    linearized_pat['dssp'] = target # assign additional column containing prediction target to the single row df\n",
    "    return linearized_pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "linear_dfs = [[linearize(pat) for pat in pats] for pats in pats_collapsed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29447, 32902, 29968, 30528, 33242, 33184, 29150]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in linear_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "linear_dfs_concat = [pd.concat(dfs, ignore_index=True) for dfs in linear_dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Save patterns as pkl object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# s = squashed, d = labelled with dssp information, x = split into groups for cross-validation, w = windowed, l = linearized\n",
    "with open('/homes/2472402/data/pssm-sdxwl.pkl', 'wb') as fp:\n",
    "    pickle.dump(linear_dfs_concat, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Do one-hot encoding of DSSP information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/2472402/miniconda3/envs/sandbox/lib/python3.9/site-packages/pandas/core/generic.py:4153: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "with open('/homes/2472402/data/pssm-sdxwl.pkl', 'rb') as fp: # list of 7 giant dataframes\n",
    "    Xs = pickle.load(fp)\n",
    "ys = [X.dssp for X in Xs] # obtain 'dssp' column from each of the 7 dfs\n",
    "Xs = [X.drop(columns='dssp') for X in Xs] # drop 'dssp' column from each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "ys = [pd.DataFrame(enc.fit_transform(y.values.reshape(-1,1))) for y in ys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Deal with NaNs in edge cases by replacing them with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values = np.nan, strategy = 'constant', fill_value = 0) # NaN with 0, a number that virtually does not occur in the squashed matrix\n",
    "Xs = [pd.DataFrame(imp.fit_transform(X)) for X in Xs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# s = squashed, d = labelled with dssp information, x = split into groups for cross-validation, w = windowed, l = linearized, i = imputed\n",
    "with open('/homes/2472402/data/pssm-sdxwli.pkl', 'wb') as fp:\n",
    "    pickle.dump([Xs,ys], fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do cross validation on split sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function to generate cross validation indices\n",
    "# custom implementation of stratifiedkfold from sklearn.model_selection\n",
    "# by altering the upper bound for\n",
    "def train_test_split(folds):\n",
    "    d_list = [] # list of dictionaries to return at end of func\n",
    "    for test_idx in range(0,folds):\n",
    "        all_idx = list(range(0,folds))\n",
    "        train_idx = [idx for idx in all_idx if idx != test_idx]\n",
    "        d = {'test_idx': test_idx, 'train_idx' : train_idx} # d is a dictionary of test and train indices\n",
    "        d_list.append(d)\n",
    "    return d_list \n",
    "\n",
    "\n",
    "# this function is copied from HMM.ipynb\n",
    "# array: numpy array\n",
    "# flank: positive integer\n",
    "def sliding_window(array, flank):\n",
    "    assert flank > 0\n",
    "    assert type(array) is np.ndarray\n",
    "    assert np.logical_not(np.isnan(np.sum(array)))\n",
    "    nrow = array.shape[0]\n",
    "    assert nrow > 0\n",
    "    ncol = array.shape[1]\n",
    "    assert ncol > 0\n",
    "    res = np.empty(shape=(nrow, (2*flank+1)*ncol))\n",
    "    res[:] = np.nan\n",
    "    for i in list(range(0,nrow)):\n",
    "        s, e = i-flank, i+flank+1\n",
    "        k = 0;\n",
    "        for j in list(range(s,e)):\n",
    "            if (j < 0 or j >= nrow):\n",
    "                res[i, k:k+ncol] = 0\n",
    "            else:\n",
    "                assert np.logical_not(np.isnan(np.sum(array[j])))\n",
    "                assert array[j].shape == (ncol,)\n",
    "                res[i, k:k+ncol] = array[j]\n",
    "            k += ncol\n",
    "    assert np.logical_not(np.isnan(np.sum(res)))\n",
    "    assert res.shape == (nrow, (2*flank+1)*ncol)\n",
    "    return res\n",
    "\n",
    "# this function rounds predictions into 1 and 0s\n",
    "def argmax(arr):\n",
    "    n, c = arr.shape\n",
    "    assert c == 3\n",
    "    assert type(arr) is np.ndarray\n",
    "    assert np.logical_not(np.isnan(np.sum(arr)))\n",
    "    res = np.empty(shape=(n,c))\n",
    "    res[:] = np.nan\n",
    "    for i in list(range(0,n)):\n",
    "        max_idx = np.argmax(arr[i])\n",
    "        if max_idx == 0:\n",
    "            res[i] = np.array([1, 0, 0])\n",
    "        elif max_idx == 1:\n",
    "            res[i] = np.array([0, 1, 0])\n",
    "        else:\n",
    "            assert max_idx == 2\n",
    "            res[i] = np.array([0, 0, 1])\n",
    "    assert np.logical_not(np.isnan(np.sum(res)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing fold 1 of cross validation at 08/06/21 14:31:49\n",
      "Fitting layer 1 model. 14:31:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating layer 1 predictions. 14:32:38\n",
      "Fitting layer 2 model. 14:32:41\n",
      "Saving results to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/fold1_model1/assets\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/fold1_model2/assets\n",
      "Finished fold 1 of cross validation at 08/06/21 14:33:30\n",
      "\n",
      "Commencing fold 2 of cross validation at 08/06/21 14:33:30\n",
      "Fitting layer 1 model. 14:33:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating layer 1 predictions. 14:34:16\n",
      "Fitting layer 2 model. 14:34:19\n",
      "Saving results to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/fold2_model1/assets\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/fold2_model2/assets\n",
      "Finished fold 2 of cross validation at 08/06/21 14:35:03\n",
      "\n",
      "Commencing fold 3 of cross validation at 08/06/21 14:35:03\n",
      "Fitting layer 1 model. 14:35:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating layer 1 predictions. 14:35:50\n",
      "Fitting layer 2 model. 14:35:52\n",
      "Saving results to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/fold3_model1/assets\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/fold3_model2/assets\n",
      "Finished fold 3 of cross validation at 08/06/21 14:36:41\n",
      "\n",
      "Commencing fold 4 of cross validation at 08/06/21 14:36:41\n",
      "Fitting layer 1 model. 14:36:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating layer 1 predictions. 14:37:29\n",
      "Fitting layer 2 model. 14:37:32\n",
      "Saving results to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/fold4_model1/assets\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/fold4_model2/assets\n",
      "Finished fold 4 of cross validation at 08/06/21 14:38:20\n",
      "\n",
      "Commencing fold 5 of cross validation at 08/06/21 14:38:20\n",
      "Fitting layer 1 model. 14:38:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating layer 1 predictions. 14:39:07\n",
      "Fitting layer 2 model. 14:39:10\n",
      "Saving results to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/fold5_model1/assets\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/fold5_model2/assets\n",
      "Finished fold 5 of cross validation at 08/06/21 14:39:59\n",
      "\n",
      "Commencing fold 6 of cross validation at 08/06/21 14:39:59\n",
      "Fitting layer 1 model. 14:39:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating layer 1 predictions. 14:40:47\n",
      "Fitting layer 2 model. 14:40:50\n",
      "Saving results to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/fold6_model1/assets\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/fold6_model2/assets\n",
      "Finished fold 6 of cross validation at 08/06/21 14:41:39\n",
      "\n",
      "Commencing fold 7 of cross validation at 08/06/21 14:41:39\n",
      "Fitting layer 1 model. 14:41:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating layer 1 predictions. 14:42:27\n",
      "Fitting layer 2 model. 14:42:30\n",
      "Saving results to /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/fold7_model1/assets\n",
      "INFO:tensorflow:Assets written to: /cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/fold7_model2/assets\n",
      "Finished fold 7 of cross validation at 08/06/21 14:43:17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from os import path\n",
    "\n",
    "# first make sure that output path is valid, otherwise computation will go to waste\n",
    "out_path = '/cluster/gjb_lab/2472402/outputs/pssm_cross_val/tmp/'\n",
    "assert path.exists(out_path)\n",
    "assert out_path[-1] == '/'\n",
    "\n",
    "# import training data\n",
    "X = pickle.load(open('/cluster/gjb_lab/2472402/data/cross-val/pssm-sdxwli-seqs-X.pkl','rb'))\n",
    "Y = pickle.load(open('/cluster/gjb_lab/2472402/data/cross-val/pssm-sdxwli-seqs-y.pkl','rb'))\n",
    "\n",
    "# comment out these lines after dry run. \n",
    "X = [x[0:2] for x in X]\n",
    "Y = [y[0:2] for y in Y]\n",
    "\n",
    "# counter for cross validation\n",
    "counter = 1\n",
    "\n",
    "# for each fold in the 7 fold cross validation procedure, do...\n",
    "for current_split in train_test_split(folds=7): \n",
    "    \n",
    "    print('Commencing fold %d of cross validation at %s'%(counter, datetime.now().strftime(\"%D %H:%M:%S\")))\n",
    "    \n",
    "    # get which set will be used for validation (test_idx), the remaining 6 will be usede for training (train_idx)\n",
    "    train_idx = current_split['train_idx']\n",
    "    test_idx = current_split['test_idx']\n",
    "    \n",
    "    # obtain test pssm profile (X) and dssp information (y)\n",
    "    X_test = np.vstack(X[test_idx])\n",
    "    Y_test = np.vstack(Y[test_idx])\n",
    "\n",
    "    assert X_test.dtype=='float64'\n",
    "    assert Y_test.dtype=='float64'\n",
    "    \n",
    "    assert X_test.shape[0] == Y_test.shape[0]\n",
    "    assert X_test.shape[1] == 340\n",
    "    assert Y_test.shape[1] == 3\n",
    "\n",
    "    # obtain train pssm profile (X) and dssp information (Y)\n",
    "    X_train = np.concatenate(tuple([X[idx] for idx in train_idx]), dtype=object)\n",
    "    Y_train = np.concatenate(tuple([Y[idx] for idx in train_idx]), dtype=object)\n",
    "    \n",
    "    X_train_stacked = np.concatenate(X_train) # dtype=float64\n",
    "    Y_train_stacked = np.concatenate(Y_train)\n",
    "    \n",
    "    assert X_train_stacked.dtype=='float64'\n",
    "    assert Y_train_stacked.dtype=='float64'\n",
    "    \n",
    "    assert X_train_stacked.shape[0] == Y_train_stacked.shape[0]\n",
    "    assert X_train_stacked.shape[1] == 340\n",
    "    assert Y_train_stacked.shape[1] == 3\n",
    "    \n",
    "    # sequence to structure layer\n",
    "    model1 = keras.Sequential([\n",
    "        layers.Dense(units=20, activation='sigmoid', input_shape=[340]),\n",
    "        layers.Dense(units=3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model1.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print('Fitting layer 1 model. %s' % datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    \n",
    "    history1 = model1.fit(X_train_stacked, Y_train_stacked,\n",
    "                          validation_data=(X_test,Y_test),\n",
    "                          batch_size=128,\n",
    "                          epochs=300, \n",
    "                          verbose=0)\n",
    "    \n",
    "    print('Calculating layer 1 predictions. %s' % datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    \n",
    "    # obtain layer 1 predictions and simplify it with argmax\n",
    "    Y_pred = [argmax(model1.predict(X)) for X in X_train]\n",
    "    \n",
    "    # convert layer 1 Y output into layer 2 X input\n",
    "    X_train_2 = [sliding_window(Y, flank=9) for Y in Y_pred]\n",
    "    X_train_2_stacked = np.concatenate(tuple(X_train_2))\n",
    "    \n",
    "    # structure to structure layer\n",
    "    model2 = keras.Sequential([\n",
    "        layers.Dense(units=20, activation='sigmoid', input_shape=[57]), \n",
    "        layers.Dense(units=3, activation = 'softmax')\n",
    "    ])\n",
    "    \n",
    "    model2.compile(\n",
    "        optimizer='sgd', \n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # feed X_test through layer 1 because we are testing it on layer 2\n",
    "    X_test = model1.predict(X_test)\n",
    "    X_test = sliding_window(X_test, flank=9)\n",
    "    \n",
    "    print('Fitting layer 2 model. %s' % datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    history2 = model2.fit(\n",
    "        X_train_2_stacked, Y_train_stacked, # note y_train is unchanged\n",
    "        validation_data=(X_test, Y_test), # this time include test data\n",
    "        batch_size=128, \n",
    "        epochs=300, \n",
    "        verbose=0, \n",
    "    )\n",
    "    \n",
    "    # save results \n",
    "    print('Saving results to %s' % out_path)\n",
    "\n",
    "    history = [pd.DataFrame(history1.history), pd.DataFrame(history2.history)]\n",
    "    pickle.dump(history, open(out_path + 'results_%d.pkl' % counter, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    model1.save(out_path + 'fold%d_model1' % counter, save_format = 'tf') # tensorflow SavedModel format\n",
    "    model2.save(out_path + 'fold%d_model2' % counter, save_format = 'tf')\n",
    "\n",
    "    # finish current fold\n",
    "    print('Finished fold %d of cross validation at %s\\n' % (counter, datetime.now().strftime(\"%D %H:%M:%S\")))\n",
    "    \n",
    "    # increment counter and continue\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80% (+/- 0.12%)\n"
     ]
    }
   ],
   "source": [
    "acc = [df.accuracy for df in results]\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(acc), np.std(acc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
